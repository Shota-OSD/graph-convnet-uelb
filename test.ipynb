{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビームサーチ＆エッジの教師データの有効性検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "exact_edges_target = create_data_files(config, data_mode=\"val\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def beam_test(net, config, master_bar, mode='val'):\n",
    "    # Set evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Assign parameters\n",
    "    num_data = getattr(config, f'num_{mode}_data')\n",
    "    #batch_size = config.batch_size\n",
    "    #num_data = 1\n",
    "    batch_size = 1\n",
    "    num_commodities = config.num_commodities\n",
    "    num_nodes = config.num_nodes\n",
    "    beam_size = config.beam_size\n",
    "    batches_per_epoch = config.batches_per_epoch\n",
    "    #batches_per_epoch = 1\n",
    "    accumulation_steps = config.accumulation_steps\n",
    "    \n",
    "    # Load UELB data\n",
    "    dataset = DatasetReader(num_data, batch_size, mode)\n",
    "    \n",
    "        # Convert dataset to iterable\n",
    "    dataset = iter(dataset)\n",
    "    \n",
    "    # Initially set loss class weights as None\n",
    "    edge_cw = None\n",
    "\n",
    "    # Initialize running data\n",
    "    running_loss = 0.0\n",
    "    running_mean_maximum_load_factor = 0.0\n",
    "    running_gt_load_factor = 0.0\n",
    "    running_nb_data = 0\n",
    "    running_nb_batch = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_test = time.time()\n",
    "        for batch_num in progress_bar(range(batches_per_epoch), parent=master_bar):\n",
    "            print(\"batch_num: \", batch_num)\n",
    "            # Generate a batch of TSPs\n",
    "            try:\n",
    "                batch = next(dataset)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            # Convert batch to torch Variables\n",
    "            x_edges_capacity = torch.FloatTensor(batch.edges_capacity).to(torch.float).contiguous().requires_grad_(False)\n",
    "            y_edges = torch.LongTensor(batch.edges_target).to(torch.long).contiguous().requires_grad_(False)\n",
    "            batch_commodities = torch.LongTensor(batch.commodities).to(torch.long).contiguous().requires_grad_(False)     \n",
    "            \n",
    "            kakai_max_values_per_batch = compute_load_factor(exact_edges_target, x_edges_capacity, batch_commodities)\n",
    "            max_values_per_batch = compute_load_factor(y_edges, x_edges_capacity, batch_commodities)\n",
    "            print(\"kakai_max_values_per_batch: \", kakai_max_values_per_batch)\n",
    "            print(\"max_values_per_batch: \", max_values_per_batch)\n",
    "\n",
    "            # Compute class weights (if uncomputed)\n",
    "            if type(edge_cw) != torch.Tensor:\n",
    "                edge_labels = y_edges.cpu().numpy().flatten()\n",
    "                edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "                \n",
    "\n",
    "            beam_search = BeamsearchUELB(\n",
    "                y_edges, beam_size, batch_size, x_edges_capacity, batch_commodities, dtypeFloat, dtypeLong, mode_strict=True) \n",
    "            #bs_nodes, pred_paths = beam_search.search()\n",
    "            pred_paths = beam_search.search()\n",
    "            torch.set_printoptions(linewidth=200)\n",
    "            \n",
    "            #if batch_num == 0:\n",
    "                #print(\"bs_nodes.shape: \", bs_nodes.shape)\n",
    "                #print(\"bs_nodes:\\n\", bs_nodes)\n",
    "                #print(\"pred_paths:\\n\", pred_paths)\n",
    "            \n",
    "            # Compute error metrics and mean load factor\n",
    "            # err_edges, err_tour, err_tsp, tour_err_idx, tsp_err_idx = edge_error(y_preds, y_edges, x_edges)\n",
    "            mean_maximum_load_factor = mean_feasible_load_factor(batch_size, num_commodities, num_nodes, pred_paths, x_edges_capacity, batch_commodities)\n",
    "            print(\"mean_maximum_load_factor: \", mean_maximum_load_factor)\n",
    "            gt_load_factor = np.mean(batch.load_factor)\n",
    "            print(\"gt_load_factor: \", gt_load_factor)\n",
    "            #if mean_maximum_load_factor < gt_load_factor:\n",
    "               #print(\"x_edges_capacity:\\n\", x_edges_capacity)\n",
    "                #print(\"batch_commodities:\\n\", batch_commodities)\n",
    "                #print(\"pred_paths:\\n\", pred_paths)\n",
    "            running_mean_maximum_load_factor += batch_size* mean_maximum_load_factor\n",
    "            running_gt_load_factor += batch_size* gt_load_factor\n",
    "            \n",
    "        #print(\"running_mean_maximum_load_factor: \", running_mean_maximum_load_factor)\n",
    "        #print(\"running_gt_load_factor: \", running_gt_load_factor)\n",
    "        Accuracy = running_gt_load_factor /running_mean_maximum_load_factor\n",
    "        print(\"Accuracy: \", Accuracy)\n",
    "        \n",
    "            \n",
    "    # Compute statistics for full epoch\n",
    "\n",
    "\n",
    "    return mean_maximum_load_factor, gt_load_factor"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
