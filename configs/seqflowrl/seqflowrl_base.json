{
    "_comment_meta": "=== SeqFlowRL: Sequential Flow Reinforcement Learning ===",
    "_description": "Hybrid approach combining GCN and Teal methods for network routing",
    "_design_decisions": "All design points confirmed - see docs/hybrid_approach_design.md",

    "expt_name": "seqflowrl_base",
    "gpu_id": "0",
    "use_gpu": true,

    "_comment_data": "=== Data Configuration ===",
    "graph_filepath": "./data/graph.gml",
    "edge_numbering_filepath": "./data/edge_numbering_file.csv",
    "train_filepath": "./data/gragh.gml",
    "val_filepath": "./data/tsp10_val_concorde.txt",
    "test_filepath": "./data/tsp10_test_concorde.txt",

    "solver_type": "pulp",
    "graph_model": "random",

    "num_train_data": 3200,
    "num_val_data": 320,
    "num_test_data": 320,
    "num_nodes": 14,
    "num_commodities": 5,
    "sample_size": 5,
    "capacity_lower": 1000,
    "capacity_higher": 10000,
    "demand_lower": 5,
    "demand_higher": 500,

    "_comment_model": "=== Model Architecture (Decision #6 confirmed) ===",
    "hidden_dim": 128,
    "num_layers": 8,
    "mlp_layers": 3,
    "aggregation": "mean",
    "dropout_rate": 0.3,

    "_comment_action": "=== Action Space (Decision #2-A confirmed) ===",
    "action_type": "node",
    "_action_type_options": ["node", "edge"],
    "_action_type_note": "node: Node-level action (default, confirmed), edge: Edge-level action (experimental)",
    "policy_head_mlp_layers": 2,

    "_comment_value": "=== Value Network (Decision #2-B confirmed) ===",
    "value_head_mlp_layers": 3,
    "value_type": "global",
    "_value_note": "Global value: all nodes × all commodities (confirmed)",

    "_comment_rollout": "=== Sequential Rollout (Decision #3 confirmed) ===",
    "gnn_update_frequency": "per_commodity",
    "_gnn_update_options": ["never", "per_commodity", "per_step"],
    "_gnn_update_note": "per_commodity: default (confirmed), never/per_step: experimental",
    "sampling_temperature": 1.0,
    "sampling_top_p": 0.9,
    "max_path_length": 20,
    "_path_note": "Maximum hops per commodity to prevent infinite loops",

    "_comment_rl": "=== RL Algorithm (Decision #4 confirmed) ===",
    "rl_algorithm": "a2c",
    "_rl_options": ["a2c", "ppo", "reinforce_baseline"],
    "_rl_note": "a2c: default (confirmed), ppo: experimental upgrade",
    "entropy_weight": 0.01,
    "value_loss_weight": 0.5,
    "gamma": 0.99,
    "_gamma_note": "Discount factor for future rewards",

    "_comment_ppo": "=== PPO Specific (Experimental) ===",
    "ppo_epsilon": 0.2,
    "ppo_epochs": 4,
    "_ppo_note": "Only used if rl_algorithm='ppo'",

    "_comment_reward": "=== Reward Function ===",
    "reward_function_version": "continuous_v1",
    "_reward_formula": "reward = 2.0 - 2.0 * load_factor",
    "_reward_range": "[-∞, 2.0], higher is better",
    "rl_use_smooth_penalty": true,
    "rl_penalty_lambda": 5.0,
    "_penalty_note": "Smooth penalty for capacity constraint violations",

    "_comment_training": "=== Training Configuration ===",
    "max_epochs": 30,
    "val_every": 10,
    "test_every": 10,
    "batch_size": 32,
    "accumulation_steps": 1,
    "learning_rate": 0.0005,
    "decay_rate": 1.2,
    "lr_scheduler": "step",
    "_lr_note": "Step LR with decay every 10 epochs",

    "_comment_pretrained": "=== Pre-trained Model (Decision #5 confirmed) ===",
    "two_phase_training": true,
    "_two_phase_note": "Phase 1: Supervised pre-training, Phase 2: RL fine-tuning (confirmed)",
    "load_pretrained_model": false,
    "pretrained_model_path": "saved_models/supervised_pretrained.pt",
    "supervised_config": "configs/gcn/supervised_pretraining.json",
    "_pretrain_note": "Set load_pretrained_model=true to use 2-phase training",

    "_comment_optimization": "=== Optimization ===",
    "optimizer": "adam",
    "adam_beta1": 0.9,
    "adam_beta2": 0.999,
    "weight_decay": 0.0001,
    "grad_clip_norm": 1.0,
    "_grad_clip_note": "Gradient clipping to prevent explosion",

    "_comment_exploration": "=== Exploration Strategy ===",
    "use_adaptive_entropy": false,
    "initial_entropy": 0.01,
    "min_entropy": 0.001,
    "entropy_decay_rate": 0.995,
    "_exploration_note": "Set use_adaptive_entropy=true for dynamic entropy adjustment",

    "_comment_advanced": "=== Advanced Options (See improvement proposals) ===",
    "use_curiosity": false,
    "curiosity_weight": 0.1,
    "epsilon_greedy": 0.0,
    "_advanced_note": "See docs/hybrid_improvement_proposals.md for details",

    "_comment_logging": "=== Logging and Checkpointing ===",
    "log_every": 10,
    "save_every": 10,
    "save_best_only": true,
    "checkpoint_dir": "saved_models/seqflowrl/",
    "tensorboard_dir": "runs/seqflowrl/",

    "_comment_eval": "=== Evaluation ===",
    "eval_deterministic": true,
    "_eval_note": "Use greedy action selection during evaluation",
    "eval_beam_search": false,
    "beam_size": 10,

    "_comment_debug": "=== Debug Options ===",
    "debug_mode": false,
    "verbose": true,
    "save_trajectories": false,
    "_debug_note": "Set debug_mode=true for detailed logging"
}
