# RL-GCN + Teal ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã®å®Ÿè£…è¨ˆç”»

**ä½œæˆæ—¥**: 2025-10-22
**ç›®çš„**: æ—¢å­˜ã®RL-GCNã¨Tealã®å¼·ã¿ã‚’çµ„ã¿åˆã‚ã›ãŸé«˜æ€§èƒ½ãªUELBè§£æ³•ã®é–‹ç™º

---

## ğŸ“Š ã‚¨ã‚°ã‚¼ã‚¯ãƒ†ã‚£ãƒ–ã‚µãƒãƒªãƒ¼

### ç¾çŠ¶åˆ†æ

**ã‚ãªãŸã®RL-GCN**:
- âœ… æ´—ç·´ã•ã‚ŒãŸResidual Gated GCN ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£
- âœ… REINFORCE + PathSampler ã«ã‚ˆã‚‹é›¢æ•£ãƒ‘ã‚¹é¸æŠ
- âœ… Beam search ã§ã®åŠ¹ç‡çš„ãªçµŒè·¯æ¢ç´¢
- âœ… ãƒãƒƒãƒå‡¦ç†å¯¾å¿œ
- âš ï¸ å°ã€œä¸­è¦æ¨¡ãƒãƒƒãƒˆãƒ¯ãƒ¼ã‚¯å‘ã‘ï¼ˆã‚¹ã‚±ãƒ¼ãƒ©ãƒ“ãƒªãƒ†ã‚£ã«èª²é¡Œï¼‰
- âš ï¸ å¯†ãªéš£æ¥è¡Œåˆ—ï¼ˆO(BÃ—VÂ²Ã—CÃ—H)ã®ãƒ¡ãƒ¢ãƒªï¼‰

**Teal (SIGCOMM '23)**:
- âœ… ç–ã‚°ãƒ©ãƒ•è¡¨ç¾ï¼ˆO(EÃ—H)ã®ãƒ¡ãƒ¢ãƒªã€ã‚¹ã‚±ãƒ¼ãƒ©ãƒ–ãƒ«ï¼‰
- âœ… é€£ç¶šãƒ•ãƒ­ãƒ¼é…åˆ†ã«ã‚ˆã‚‹æœ€é©åŒ–
- âœ… ADMMåˆ¶ç´„å‡¦ç†ï¼ˆå®¹é‡ãƒ»éœ€è¦åˆ¶ç´„ã®è‡ªå‹•èª¿æ•´ï¼‰
- âœ… COMAå¼å ±é…¬æ¨å®šï¼ˆåˆ†æ•£å‰Šæ¸›ï¼‰
- âœ… å¤§è¦æ¨¡WANå¯¾å¿œï¼ˆ6,474ãƒãƒ¼ãƒ‰ã§å®Ÿè¨¼ï¼‰
- âš ï¸ ã‚·ãƒ³ãƒ—ãƒ«ãªGNNï¼ˆFlowGNN = ç·šå½¢å±¤ + ã‚¹ãƒ‘ãƒ¼ã‚¹è¡Œåˆ—ç©ï¼‰

### ææ¡ˆæ‰‹æ³•

**ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰ã‚¢ãƒ—ãƒ­ãƒ¼ãƒ**: æ—¢å­˜ã®ResidualGatedGCNã‚’ç¶­æŒã—ã€Tealã®ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã‚’çµ±åˆ

```
æ—¢å­˜ RL-GCN (ç¶­æŒ)          TealæŠ€è¡“ (è¿½åŠ )
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”        â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ ResidualGatedGCNâ”‚   â†’    â”‚ Flow Allocation  â”‚
â”‚ (Batchå‡¦ç†)      â”‚        â”‚ ADMM Constraints â”‚
â”‚ å¯†ãªéš£æ¥è¡Œåˆ—      â”‚        â”‚ Flow Rounding    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜        â”‚ COMA Rewards     â”‚
                           â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

**æœŸå¾…ã•ã‚Œã‚‹åŠ¹æœ**:
- ğŸ¯ åˆ¶ç´„æº€è¶³ç‡ã®å‘ä¸Šï¼ˆADMMã«ã‚ˆã‚‹è‡ªå‹•èª¿æ•´ï¼‰
- ğŸ¯ å­¦ç¿’ã®å®‰å®šåŒ–ï¼ˆCOMAå ±é…¬æ¨å®šï¼‰
- ğŸ¯ æœ€é©è§£ã®å“è³ªå‘ä¸Šï¼ˆãƒ•ãƒ­ãƒ¼é…åˆ†ã®æ´—ç·´åŒ–ï¼‰
- ğŸ¯ å°†æ¥çš„ãªã‚¹ã‚±ãƒ¼ãƒ«æ‹¡å¼µã®åŸºç›¤

---

## ğŸ” æŠ€è¡“çš„èƒŒæ™¯

### ResidualGatedGCN vs Teal FlowGNN

| é …ç›® | ResidualGatedGCN | Teal FlowGNN |
|------|------------------|--------------|
| **ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£** | Gatingæ©Ÿæ§‹ + Residual | å˜ç´”ãªç·šå½¢å±¤ |
| **ã‚°ãƒ©ãƒ•è¡¨ç¾** | å¯†ãªéš£æ¥è¡Œåˆ— (BÃ—VÃ—VÃ—CÃ—H) | ç–ãªCOOå½¢å¼ (EÃ—H) |
| **ãƒ¡ãƒ¢ãƒªåŠ¹ç‡** | O(BÃ—VÂ²Ã—CÃ—H) | O(EÃ—H) |
| **ãƒãƒƒãƒå‡¦ç†** | âœ… ã‚ã‚Š | âŒ ãªã— |
| **ã‚³ãƒ¢ãƒ‡ã‚£ãƒ†ã‚£å‡¦ç†** | æ˜ç¤ºçš„ã«åˆ†é›¢ | ãƒ‘ã‚¹ãƒãƒ¼ãƒ‰ã¨ã—ã¦çµ±åˆ |
| **ã‚¨ãƒƒã‚¸æ›´æ–°** | âœ… åŒæ–¹å‘æ›´æ–° | âš ï¸ å˜æ–¹å‘æ›´æ–° |
| **æ­£è¦åŒ–** | BatchNorm | ãªã— |
| **è¡¨ç¾åŠ›** | é«˜ã„ | ä½ã„ |
| **ã‚¹ã‚±ãƒ¼ãƒ«** | å°ã€œä¸­è¦æ¨¡ | å¤§è¦æ¨¡ |

### ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®æ¯”è¼ƒ

**å°è¦æ¨¡ (10ãƒãƒ¼ãƒ‰ã€5ã‚³ãƒ¢ãƒ‡ã‚£ãƒ†ã‚£ã€ãƒãƒƒãƒ32)**:
- ResidualGatedGCN: ç´„ 9 MB
- FlowGNN: ç´„ 76 KB
- **å‰Šæ¸›ç‡: 120å€**

**ä¸­è¦æ¨¡ (50ãƒãƒ¼ãƒ‰ã€20ã‚³ãƒ¢ãƒ‡ã‚£ãƒ†ã‚£ã€ãƒãƒƒãƒ32)**:
- ResidualGatedGCN: ç´„ 1.6 GB
- FlowGNN: ç´„ 500 KB
- **å‰Šæ¸›ç‡: 3,200å€**

**å¤§è¦æ¨¡ (100ãƒãƒ¼ãƒ‰ã€50ã‚³ãƒ¢ãƒ‡ã‚£ãƒ†ã‚£ã€ãƒãƒƒãƒ32)**:
- ResidualGatedGCN: ç´„ 32 GBï¼ˆGPU1å°ã«è¼‰ã‚‰ãªã„ï¼‰
- FlowGNN: ç´„ 1 MB
- **å‰Šæ¸›ç‡: 32,000å€**

### Tealã®ä¸»è¦æŠ€è¡“

#### 1. ADMM (Alternating Direction Method of Multipliers)

**ç›®çš„**: å®¹é‡ãƒ»éœ€è¦åˆ¶ç´„é•åã‚’åå¾©çš„ã«ä¿®æ­£

```python
# Tealã®ã‚³ãƒ¼ãƒ‰ï¼ˆå‚è€ƒï¼‰
for iteration in range(num_admm_steps):
    # 1. å®¹é‡åˆ¶ç´„é•åã‚’è¨ˆç®—
    edge_flow = aggregate_path_flow_to_edges(path_flow)
    util = edge_flow / capacity

    # 2. é•ååº¦ã«å¿œã˜ã¦ãƒ•ãƒ­ãƒ¼èª¿æ•´
    violation_factor = relu(util - 1.0) + 1.0
    path_adjustment = compute_adjustment(violation_factor)

    # 3. ãƒ•ãƒ­ãƒ¼ã‚’æ›´æ–°
    path_flow = path_flow - learning_rate * path_adjustment
```

**åŠ¹æœ**:
- åˆ¶ç´„é•åã‚’è‡ªå‹•çš„ã«ä¿®æ­£
- å®Ÿç¾å¯èƒ½è§£ã®ç”Ÿæˆç‡ãŒå‘ä¸Š
- æœ€é©åŒ–ã®åæŸæ€§ãŒæ”¹å–„

#### 2. COMA (Counterfactual Multi-Agent) å ±é…¬æ¨å®š

**å•é¡Œ**: Policy Gradientã¯åˆ†æ•£ãŒå¤§ãã„

**è§£æ±ºç­–**: åäº‹å®Ÿçš„ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã§åˆ†æ•£å‰Šæ¸›

```python
# ç¾åœ¨ã®è¡Œå‹•ã®å ±é…¬
current_reward = env.step(current_action)

# ä»£æ›¿è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦å¹³å‡å ±é…¬ã‚’è¨ˆç®—
baseline_reward = 0
for _ in range(num_samples):
    alt_action = sample_alternative_action()
    baseline_reward += env.step(alt_action)
baseline_reward /= num_samples

# Advantage = ç¾åœ¨ã®å ±é…¬ - ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³
advantage = current_reward - baseline_reward
loss = -log_prob * advantage
```

**åŠ¹æœ**:
- å‹¾é…ã®åˆ†æ•£ã‚’å¤§å¹…å‰Šæ¸›
- å­¦ç¿’ã®å®‰å®šåŒ–
- åæŸé€Ÿåº¦ã®å‘ä¸Š

#### 3. Flow Rounding

**ç›®çš„**: é€£ç¶šãƒ•ãƒ­ãƒ¼é…åˆ†ã‚’å®Ÿç¾å¯èƒ½è§£ã«å¤‰æ›

```python
def round_flow(path_flow, capacity, demand):
    # Step 1: éœ€è¦åˆ¶ç´„ã‚’æº€ãŸã™ã‚ˆã†æ­£è¦åŒ–
    for commodity in commodities:
        total_flow = sum(path_flow[commodity])
        if total_flow > demand[commodity]:
            path_flow[commodity] *= demand[commodity] / total_flow

    # Step 2: å®¹é‡åˆ¶ç´„ã‚’æº€ãŸã™ã‚ˆã†åå¾©å‰Šæ¸›
    for iteration in range(num_iterations):
        edge_flow = aggregate_to_edges(path_flow)
        violation_ratio = edge_flow / capacity

        for path in paths:
            max_violation = max(violation_ratio[edges_in_path])
            path_flow[path] /= max_violation

    return path_flow
```

**åŠ¹æœ**:
- å¿…ãšå®Ÿç¾å¯èƒ½ãªè§£ã‚’ç”Ÿæˆ
- æœ€é©è§£ã«è¿‘ã„å“è³ªã‚’ç¶­æŒ

---

## ğŸ¯ ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æ‰‹æ³•ã®è¨­è¨ˆ

### ã‚¢ãƒ—ãƒ­ãƒ¼ãƒé¸æŠ: ResidualGatedGCN + Teal Flow Frameworkï¼ˆæ¨å¥¨ï¼‰

**ç†ç”±**:
1. æ—¢å­˜ã®GCNã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£ã‚’ç¶­æŒï¼ˆå†å®Ÿè£…ä¸è¦ï¼‰
2. ãƒãƒƒãƒå‡¦ç†ã‚’ç¶­æŒï¼ˆè¨“ç·´åŠ¹ç‡ï¼‰
3. Tealã®å®Ÿè¨¼æ¸ˆã¿ãƒ†ã‚¯ãƒ‹ãƒƒã‚¯ã®ã¿å°å…¥ï¼ˆãƒªã‚¹ã‚¯ä½æ¸›ï¼‰
4. æ®µéšçš„ãªå®Ÿè£…ãƒ»æ¤œè¨¼ãŒå¯èƒ½

### ã‚·ã‚¹ãƒ†ãƒ ã‚¢ãƒ¼ã‚­ãƒ†ã‚¯ãƒãƒ£

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Input: Graph topology, Commodities, Capacities         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 1: Feature Extraction (æ—¢å­˜)                      â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ ResidualGatedGCNModel                             â”‚   â”‚
â”‚ â”‚ - Embedding layers                                â”‚   â”‚
â”‚ â”‚ - 3 x ResidualGatedGCNLayer                       â”‚   â”‚
â”‚ â”‚   - Gating mechanism                              â”‚   â”‚
â”‚ â”‚   - Residual connections                          â”‚   â”‚
â”‚ â”‚   - Batch normalization                           â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Output: Edge scores (B, V, V, C)                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 2: Flow Allocation (æ–°è¦)                         â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ FlowAllocationLayer                               â”‚   â”‚
â”‚ â”‚ - Convert edge scores â†’ path flows                â”‚   â”‚
â”‚ â”‚ - Apply softmax per commodity                     â”‚   â”‚
â”‚ â”‚ - Multiply by demands                             â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Output: Path flows (B, C, K) K=num_paths               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 3: Constraint Handling (æ–°è¦)                     â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ SimpleFlowRounder (Phase 1A)                      â”‚   â”‚
â”‚ â”‚ - Round demand constraints                        â”‚   â”‚
â”‚ â”‚ - Round capacity constraints                      â”‚   â”‚
â”‚ â”‚                                                    â”‚   â”‚
â”‚ â”‚ ADMMConstraintHandler (Phase 1B)                  â”‚   â”‚
â”‚ â”‚ - Iterative ADMM refinement                       â”‚   â”‚
â”‚ â”‚ - Dual variable updates                           â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚ Output: Feasible path flows (B, C, K)                  â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                     â”‚
                     â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Phase 4: Reward Computation & Learning (æ”¹è‰¯)           â”‚
â”‚ â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚ â”‚ FlowRewardComputer                                â”‚   â”‚
â”‚ â”‚ - Load factor (existing)                          â”‚   â”‚
â”‚ â”‚ - Total flow (Teal)                               â”‚   â”‚
â”‚ â”‚ - Constraint violation penalty                    â”‚   â”‚
â”‚ â”‚                                                    â”‚   â”‚
â”‚ â”‚ COMARewardEstimator (Phase 1B)                    â”‚   â”‚
â”‚ â”‚ - Counterfactual baseline                         â”‚   â”‚
â”‚ â”‚ - Advantage computation                           â”‚   â”‚
â”‚ â”‚                                                    â”‚   â”‚
â”‚ â”‚ Policy Gradient Loss                              â”‚   â”‚
â”‚ â”‚ loss = -log_prob * advantage                      â”‚   â”‚
â”‚ â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ“ å®Ÿè£…è¨ˆç”»

### Phase 1A: æœ€å°å®Ÿè£…ï¼ˆ2é€±é–“ï¼‰

**ç›®æ¨™**: åŸºæœ¬çš„ãªãƒ•ãƒ­ãƒ¼é…åˆ†æ©Ÿèƒ½ã‚’å®Ÿè£…ã—ã€æ—¢å­˜RL-GCNã¨åŒç­‰æ€§èƒ½ã‚’ç¢ºèª

#### 1. FlowAllocationLayer

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gcn_flow/models/flow_allocation_layer.py`

```python
class FlowAllocationLayer(nn.Module):
    """
    GCNã®ã‚¨ãƒƒã‚¸ã‚¹ã‚³ã‚¢ã‚’ãƒ‘ã‚¹ã”ã¨ã®ãƒ•ãƒ­ãƒ¼é…åˆ†ã«å¤‰æ›

    Input:
        edge_scores: (B, V, V, C) - GCNã®å‡ºåŠ›
        paths: List[(src, dst, [path_nodes])] - å€™è£œãƒ‘ã‚¹
        demands: (B, C) - ã‚³ãƒ¢ãƒ‡ã‚£ãƒ†ã‚£éœ€è¦

    Output:
        path_flows: (B, num_paths) - å„ãƒ‘ã‚¹ã®ãƒ•ãƒ­ãƒ¼
        log_probs: (B,) - ãƒ­ã‚°ç¢ºç‡ï¼ˆPolicy Gradientç”¨ï¼‰
    """
```

**å®Ÿè£…å†…å®¹**:
- ã‚¨ãƒƒã‚¸ã‚¹ã‚³ã‚¢ â†’ ãƒ‘ã‚¹ã‚¹ã‚³ã‚¢é›†ç´„
- Softmax per commodity
- éœ€è¦ã¨ã®ä¹—ç®—
- ãƒ­ã‚°ç¢ºç‡ã®è¨ˆç®—

#### 2. SimpleFlowRounder

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gcn_flow/algorithms/flow_rounder.py`

```python
class SimpleFlowRounder:
    """
    åŸºæœ¬çš„ãªãƒ•ãƒ­ãƒ¼ä¸¸ã‚å‡¦ç†

    Tealã®round_flow()ã‚’ç°¡ç•¥åŒ–ã—ãŸç‰ˆ
    """

    def round(self, path_flows, capacity, demands):
        # 1. éœ€è¦åˆ¶ç´„ã®å¼·åˆ¶
        # 2. å®¹é‡åˆ¶ç´„ã®å¼·åˆ¶ï¼ˆ2å›åå¾©ï¼‰
        return feasible_flows
```

#### 3. FlowRewardComputer

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gcn_flow/utils/metrics.py`

```python
class FlowRewardComputer:
    """
    è¤‡åˆå ±é…¬é–¢æ•°ã®è¨ˆç®—

    reward = Î± * load_factor_reward
           + Î² * flow_utilization_reward
           - Î³ * constraint_violation_penalty
    """

    def compute_reward(self, flows, capacities, demands):
        # Load factor (from RL-GCN)
        load_reward = -compute_max_load_factor(flows, capacities)

        # Total flow (from Teal)
        flow_reward = flows.sum() / demands.sum()

        # Violation penalty
        violation = self._compute_violation(flows, capacities, demands)
        penalty = -violation

        return self.alpha * load_reward + \
               self.beta * flow_reward + \
               self.gamma * penalty
```

#### 4. HybridRLStrategy (åŸºæœ¬ç‰ˆ)

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gcn_flow/training/hybrid_rl_strategy.py`

```python
class HybridRLStrategy(BaseTrainingStrategy):
    """
    RL-GCN + Teal ãƒ•ãƒ­ãƒ¼æœ€é©åŒ–ã®ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥

    Phase 1A: REINFORCE + Simple Rounding
    Phase 1B: COMA + ADMM
    """

    def __init__(self, config):
        super().__init__(config)

        # Flow allocation
        self.flow_layer = FlowAllocationLayer(...)

        # Constraint handling
        self.flow_rounder = SimpleFlowRounder()

        # Reward computation
        self.reward_computer = FlowRewardComputer(
            alpha=config.get('reward_alpha', 1.0),
            beta=config.get('reward_beta', 0.1),
            gamma=config.get('reward_gamma', 5.0)
        )

    def compute_loss(self, model, batch_data, device):
        # 1. GCN forward
        edge_scores, _ = model.forward(...)

        # 2. Flow allocation
        path_flows, log_probs = self.flow_layer(
            edge_scores, paths, demands
        )

        # 3. Round to feasible solution
        feasible_flows = self.flow_rounder.round(
            path_flows, capacities, demands
        )

        # 4. Compute reward
        reward = self.reward_computer.compute_reward(
            feasible_flows, capacities, demands
        )

        # 5. Policy gradient loss (REINFORCE)
        advantage = reward - self.baseline
        loss = -(log_probs * advantage.detach()).mean()

        # 6. Update baseline
        self.baseline = 0.9 * self.baseline + 0.1 * reward.mean()

        metrics = {
            'loss': loss.item(),
            'reward': reward.mean().item(),
            'advantage': advantage.mean().item(),
            'max_load': compute_max_load_factor(feasible_flows, capacities),
            'total_flow': feasible_flows.sum().item(),
        }

        return loss, metrics
```

#### ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ

- [ ] FlowAllocationLayer ã®å®Ÿè£…
- [ ] SimpleFlowRounder ã®å®Ÿè£…
- [ ] FlowRewardComputer ã®å®Ÿè£…
- [ ] HybridRLStrategy ã®å®Ÿè£…
- [ ] PathGenerator ãƒ¦ãƒ¼ãƒ†ã‚£ãƒªãƒ†ã‚£ï¼ˆk-shortest pathsï¼‰
- [ ] å˜ä½“ãƒ†ã‚¹ãƒˆã®ä½œæˆ
- [ ] æ—¢å­˜RL-GCNã¨ã®æ€§èƒ½æ¯”è¼ƒå®Ÿé¨“

**æœŸå¾…ã•ã‚Œã‚‹æˆæœ**:
- æ—¢å­˜RL-GCNã¨åŒç­‰ã®æ€§èƒ½
- ãƒ•ãƒ­ãƒ¼é…åˆ†ã®å¯è¦–åŒ–
- ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ç¢ºç«‹

---

### Phase 1B: Tealçµ±åˆï¼ˆ2é€±é–“ï¼‰

**ç›®æ¨™**: ADMMã¨COMAã‚’å°å…¥ã—ã€æ€§èƒ½å‘ä¸Šã‚’ç¢ºèª

#### 5. ADMMConstraintHandler

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gcn_flow/algorithms/admm_handler.py`

```python
class ADMMConstraintHandler:
    """
    Teal-style ADMMåˆ¶ç´„å‡¦ç†

    Augmented Lagrangian:
    L(x, Î», Ï) = f(x) + Î»áµ€(Ax-b) + (Ï/2)||Ax-b||Â²
    """

    def __init__(self, rho=1.0, num_iterations=5):
        self.rho = rho  # ãƒšãƒŠãƒ«ãƒ†ã‚£ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿
        self.num_iterations = num_iterations
        self.dual_vars = None  # ãƒ©ã‚°ãƒ©ãƒ³ã‚¸ãƒ¥ä¹—æ•°

    def refine(self, path_flows, p2e_mapping, capacities, demands):
        """
        ADMMåå¾©ã«ã‚ˆã‚‹ãƒ•ãƒ­ãƒ¼èª¿æ•´

        Args:
            path_flows: (B, num_paths) - åˆæœŸãƒ•ãƒ­ãƒ¼
            p2e_mapping: (2, num_path_edge_pairs) - ãƒ‘ã‚¹â†’ã‚¨ãƒƒã‚¸ãƒãƒƒãƒ”ãƒ³ã‚°
            capacities: (B, num_edges) - ã‚¨ãƒƒã‚¸å®¹é‡
            demands: (B, num_commodities) - éœ€è¦

        Returns:
            refined_flows: (B, num_paths) - èª¿æ•´å¾Œãƒ•ãƒ­ãƒ¼
        """
        x = path_flows.clone()

        # Initialize dual variables
        if self.dual_vars is None:
            num_edges = capacities.shape[1]
            self.dual_vars = torch.zeros(capacities.shape)

        for iteration in range(self.num_iterations):
            # 1. Compute edge flows
            edge_flows = self._aggregate_to_edges(x, p2e_mapping)

            # 2. Compute constraint violations
            capacity_violation = F.relu(edge_flows - capacities)

            # 3. Update dual variables
            self.dual_vars += self.rho * capacity_violation

            # 4. Update primal variables (path flows)
            # Gradient of augmented Lagrangian
            gradient = self._compute_gradient(
                x, edge_flows, capacities, self.dual_vars, p2e_mapping
            )
            x = x - (1.0 / self.rho) * gradient

            # 5. Project to feasible set
            x = self._project_to_demands(x, demands)
            x = F.relu(x)  # Non-negative flows

        return x
```

#### 6. COMARewardEstimator

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gcn_flow/algorithms/coma_reward.py`

```python
class COMARewardEstimator:
    """
    Counterfactual Multi-Agent å ±é…¬æ¨å®š

    åˆ†æ•£å‰Šæ¸›ã®ãŸã‚ã€ä»£æ›¿è¡Œå‹•ã‚’ã‚µãƒ³ãƒ—ãƒªãƒ³ã‚°ã—ã¦
    ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã‚’è¨ˆç®—
    """

    def __init__(self, num_samples=5):
        self.num_samples = num_samples

    def estimate_advantage(self, model, batch_data, current_action, current_reward):
        """
        Args:
            model: GCNãƒ¢ãƒ‡ãƒ«
            batch_data: ãƒãƒƒãƒãƒ‡ãƒ¼ã‚¿
            current_action: ç¾åœ¨ã®è¡Œå‹•ï¼ˆãƒ‘ã‚¹ãƒ•ãƒ­ãƒ¼ï¼‰
            current_reward: ç¾åœ¨ã®å ±é…¬

        Returns:
            advantage: current_reward - baseline_reward
        """
        baseline_rewards = []

        with torch.no_grad():
            for _ in range(self.num_samples):
                # Alternative action:
                # Option 1: Sample from uniform distribution
                alt_action = torch.rand_like(current_action)
                alt_action = self._normalize_to_demands(alt_action, batch_data)

                # Option 2: Sample from model with noise
                # noise = torch.randn_like(current_action) * 0.1
                # alt_action = current_action + noise

                # Compute reward for alternative action
                alt_reward = self._compute_reward(alt_action, batch_data)
                baseline_rewards.append(alt_reward)

        baseline = torch.stack(baseline_rewards).mean(dim=0)
        advantage = current_reward - baseline

        return advantage
```

#### 7. HybridRLStrategy ã®æ‹¡å¼µ

```python
class HybridRLStrategy(BaseTrainingStrategy):
    def __init__(self, config):
        # ... (Phase 1Aã®å†…å®¹)

        # Phase 1B additions
        if config.get('use_admm', True):
            self.admm_handler = ADMMConstraintHandler(
                rho=config.get('admm_rho', 1.0),
                num_iterations=config.get('admm_iterations', 5)
            )

        if config.get('use_coma', True):
            self.coma_estimator = COMARewardEstimator(
                num_samples=config.get('coma_samples', 5)
            )

    def compute_loss(self, model, batch_data, device):
        # 1-2. Same as Phase 1A
        edge_scores, _ = model.forward(...)
        path_flows, log_probs = self.flow_layer(...)

        # 3. ADMM refinement (instead of simple rounding)
        if hasattr(self, 'admm_handler'):
            feasible_flows = self.admm_handler.refine(
                path_flows, p2e_mapping, capacities, demands
            )
        else:
            feasible_flows = self.flow_rounder.round(...)

        # 4. Compute reward
        reward = self.reward_computer.compute_reward(...)

        # 5. COMA advantage estimation
        if hasattr(self, 'coma_estimator'):
            advantage = self.coma_estimator.estimate_advantage(
                model, batch_data, feasible_flows, reward
            )
        else:
            advantage = reward - self.baseline
            self.baseline = 0.9 * self.baseline + 0.1 * reward.mean()

        # 6. Policy gradient loss
        loss = -(log_probs * advantage.detach()).mean()

        return loss, metrics
```

#### ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ

- [ ] ADMMConstraintHandler ã®å®Ÿè£…
- [ ] COMARewardEstimator ã®å®Ÿè£…
- [ ] HybridRLStrategy ã®æ‹¡å¼µ
- [ ] ADMMåæŸæ€§ã®ãƒ†ã‚¹ãƒˆ
- [ ] COMAåˆ†æ•£å‰Šæ¸›åŠ¹æœã®æ¤œè¨¼
- [ ] Phase 1Aã¨ã®æ€§èƒ½æ¯”è¼ƒå®Ÿé¨“

**æœŸå¾…ã•ã‚Œã‚‹æˆæœ**:
- åˆ¶ç´„æº€è¶³ç‡ã®å‘ä¸Šï¼ˆ90% â†’ 98%+ï¼‰
- å­¦ç¿’ã®å®‰å®šåŒ–ï¼ˆå‹¾é…åˆ†æ•£ã®å‰Šæ¸›ï¼‰
- æœ€é©è§£ã¸ã®è¿‘ä¼¼åº¦å‘ä¸Š

---

### Phase 2: ã‚¹ã‚±ãƒ¼ãƒ«å¯¾å¿œï¼ˆ2é€±é–“ï¼‰

**ç›®æ¨™**: ã‚ˆã‚Šå¤§è¦æ¨¡ãªå•é¡Œã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã¸ã®å¯¾å¿œ

#### 8. GraphConverter

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gcn_flow/utils/graph_converter.py`

```python
class GraphConverter:
    """
    å¯†ãªéš£æ¥è¡Œåˆ— â‡” ç–ãªCOOå½¢å¼ã®å¤‰æ›

    å¤§è¦æ¨¡å•é¡Œã§ã¯ç–è¡¨ç¾ã«å¤‰æ›ã—ã¦ãƒ¡ãƒ¢ãƒªå‰Šæ¸›
    """

    def dense_to_sparse(self, adjacency_matrix, capacity_matrix):
        """
        Args:
            adjacency_matrix: (B, V, V)
            capacity_matrix: (B, V, V)

        Returns:
            edge_index: (2, E) - COO format
            edge_attr: (E, features)
        """
        pass

    def sparse_to_dense(self, edge_index, edge_attr, num_nodes):
        """Reverse conversion"""
        pass
```

#### 9. EfficientPathGenerator

**ãƒ•ã‚¡ã‚¤ãƒ«**: `src/gcn_flow/utils/path_generator.py`

```python
class EfficientPathGenerator:
    """
    åŠ¹ç‡çš„ãªk-shortest pathç”Ÿæˆ

    Yen's algorithmã¾ãŸã¯Suurballeã®ã‚¢ãƒ«ã‚´ãƒªã‚ºãƒ ã‚’ä½¿ç”¨
    """

    def generate_paths(self, graph, commodities, k=4, edge_disjoint=False):
        """
        Args:
            graph: NetworkX graph
            commodities: List[(src, dst, demand)]
            k: Number of paths per commodity
            edge_disjoint: Whether to find edge-disjoint paths

        Returns:
            path_dict: {(src, dst): [path1, path2, ..., pathk]}
        """
        pass
```

#### ã‚¿ã‚¹ã‚¯ãƒªã‚¹ãƒˆ

- [ ] GraphConverter ã®å®Ÿè£…
- [ ] EfficientPathGenerator ã®å®Ÿè£…
- [ ] ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°
- [ ] å¤§è¦æ¨¡ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã§ã®ãƒ†ã‚¹ãƒˆï¼ˆ50+ ãƒãƒ¼ãƒ‰ï¼‰
- [ ] ãƒãƒƒãƒã‚µã‚¤ã‚º vs ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®ãƒˆãƒ¬ãƒ¼ãƒ‰ã‚ªãƒ•åˆ†æ

**æœŸå¾…ã•ã‚Œã‚‹æˆæœ**:
- 50ãƒãƒ¼ãƒ‰ä»¥ä¸Šã®å•é¡Œã«å¯¾å¿œ
- ãƒ¡ãƒ¢ãƒªä½¿ç”¨é‡ã®å¤§å¹…å‰Šæ¸›
- ã‚ˆã‚Šç¾å®Ÿçš„ãªè¦æ¨¡ã§ã®è©•ä¾¡

---

## ğŸ“Š è©•ä¾¡è¨ˆç”»

### ãƒ™ãƒ¼ã‚¹ãƒ©ã‚¤ãƒ³ã¨ã®æ¯”è¼ƒ

| æ‰‹æ³• | Phase 1A | Phase 1B | Phase 2 |
|------|----------|----------|---------|
| **æ—¢å­˜RL-GCN** | âœ“ | âœ“ | âœ“ |
| **Supervised GCN** | âœ“ | âœ“ | âœ“ |
| **Gurobi (æœ€é©è§£)** | âœ“ | âœ“ | âœ“ |
| **Teal (if applicable)** | - | - | âœ“ |

### è©•ä¾¡æŒ‡æ¨™

**ä¸»è¦æŒ‡æ¨™**:
1. **æœ€å¤§è² è·ç‡ (Max Load Factor)**: å°ã•ã„ã»ã©è‰¯ã„
2. **å®Ÿè¡Œæ™‚é–“**: é€Ÿã„ã»ã©è‰¯ã„
3. **åˆ¶ç´„æº€è¶³ç‡**: é«˜ã„ã»ã©è‰¯ã„ï¼ˆç›®æ¨™: 95%+ï¼‰

**å‰¯æ¬¡æŒ‡æ¨™**:
4. **ç·ãƒ•ãƒ­ãƒ¼**: å¤§ãã„ã»ã©è‰¯ã„
5. **å­¦ç¿’æ›²ç·š**: å®‰å®šã—ã¦ã„ã‚‹ã»ã©è‰¯ã„
6. **åæŸã‚¨ãƒãƒƒã‚¯æ•°**: å°‘ãªã„ã»ã©è‰¯ã„

### å®Ÿé¨“ã‚»ãƒƒãƒˆã‚¢ãƒƒãƒ—

**å°è¦æ¨¡å•é¡Œ** (Phase 1A/1B):
- ãƒãƒ¼ãƒ‰æ•°: 10-20
- ã‚³ãƒ¢ãƒ‡ã‚£ãƒ†ã‚£æ•°: 5-10
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 32
- ã‚¨ãƒãƒƒã‚¯æ•°: 100

**ä¸­è¦æ¨¡å•é¡Œ** (Phase 1B/2):
- ãƒãƒ¼ãƒ‰æ•°: 20-50
- ã‚³ãƒ¢ãƒ‡ã‚£ãƒ†ã‚£æ•°: 10-30
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 16
- ã‚¨ãƒãƒƒã‚¯æ•°: 150

**å¤§è¦æ¨¡å•é¡Œ** (Phase 2):
- ãƒãƒ¼ãƒ‰æ•°: 50-100
- ã‚³ãƒ¢ãƒ‡ã‚£ãƒ†ã‚£æ•°: 30-100
- ãƒãƒƒãƒã‚µã‚¤ã‚º: 4-8
- ã‚¨ãƒãƒƒã‚¯æ•°: 200

---

## ğŸ“‚ ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªæ§‹é€ 

```
src/gcn_flow/
â”œâ”€â”€ models/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ flow_gnn.py                 # âœ… å®Ÿè£…æ¸ˆã¿ï¼ˆå‚è€ƒç”¨ï¼‰
â”‚   â”œâ”€â”€ flow_actor.py               # âœ… å®Ÿè£…æ¸ˆã¿ï¼ˆå‚è€ƒç”¨ï¼‰
â”‚   â”œâ”€â”€ flow_allocation_layer.py    # ğŸ†• Phase 1A - ãƒ‘ã‚¹ãƒ•ãƒ­ãƒ¼é…åˆ†
â”‚   â””â”€â”€ hybrid_gcn_model.py         # ğŸ†• Phase 1A - çµ±åˆãƒ¢ãƒ‡ãƒ«
â”‚
â”œâ”€â”€ algorithms/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ flow_env.py                 # âœ… å®Ÿè£…æ¸ˆã¿ï¼ˆå‚è€ƒç”¨ï¼‰
â”‚   â”œâ”€â”€ flow_rounder.py             # ğŸ†• Phase 1A - Simple rounding
â”‚   â”œâ”€â”€ admm_handler.py             # ğŸ†• Phase 1B - ADMMåˆ¶ç´„å‡¦ç†
â”‚   â””â”€â”€ coma_reward.py              # ğŸ†• Phase 1B - COMAå ±é…¬æ¨å®š
â”‚
â”œâ”€â”€ training/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ hybrid_rl_strategy.py       # ğŸ†• Phase 1A - ãƒã‚¤ãƒ–ãƒªãƒƒãƒ‰æˆ¦ç•¥
â”‚   â””â”€â”€ flow_trainer.py             # âœ… å®Ÿè£…æ¸ˆã¿ï¼ˆè¦æ”¹è‰¯ï¼‰
â”‚
â”œâ”€â”€ utils/
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ metrics.py                  # ğŸ†• Phase 1A - å ±é…¬è¨ˆç®—
â”‚   â”œâ”€â”€ path_generator.py           # ğŸ†• Phase 1A/2 - ãƒ‘ã‚¹ç”Ÿæˆ
â”‚   â”œâ”€â”€ graph_converter.py          # ğŸ†• Phase 2 - ã‚°ãƒ©ãƒ•å¤‰æ›
â”‚   â””â”€â”€ visualization.py            # ğŸ†• Optional - å¯è¦–åŒ–
â”‚
â”œâ”€â”€ configs/
â”‚   â”œâ”€â”€ hybrid_rl_phase1a.json      # ğŸ†• Phase 1Aè¨­å®š
â”‚   â”œâ”€â”€ hybrid_rl_phase1b.json      # ğŸ†• Phase 1Bè¨­å®š
â”‚   â””â”€â”€ hybrid_rl_phase2.json       # ğŸ†• Phase 2è¨­å®š
â”‚
â”œâ”€â”€ tests/
â”‚   â”œâ”€â”€ test_flow_allocation.py     # ğŸ†• å˜ä½“ãƒ†ã‚¹ãƒˆ
â”‚   â”œâ”€â”€ test_admm.py                # ğŸ†• å˜ä½“ãƒ†ã‚¹ãƒˆ
â”‚   â””â”€â”€ test_coma.py                # ğŸ†• å˜ä½“ãƒ†ã‚¹ãƒˆ
â”‚
â””â”€â”€ README.md                        # âœ… å®Ÿè£…æ¸ˆã¿
```

---

## ğŸš€ å®Ÿè£…ã‚¹ã‚±ã‚¸ãƒ¥ãƒ¼ãƒ«

### Week 1-2: Phase 1A (æœ€å°å®Ÿè£…)

**Week 1**:
- Day 1-2: FlowAllocationLayer å®Ÿè£…
- Day 3-4: SimpleFlowRounder å®Ÿè£…
- Day 5: FlowRewardComputer å®Ÿè£…

**Week 2**:
- Day 1-2: HybridRLStrategy å®Ÿè£…
- Day 3-4: PathGenerator å®Ÿè£…
- Day 5: çµ±åˆãƒ†ã‚¹ãƒˆ & ãƒ‡ãƒãƒƒã‚°

**Milestone**: æ—¢å­˜RL-GCNã¨åŒç­‰æ€§èƒ½ã‚’é”æˆ

### Week 3-4: Phase 1B (Tealçµ±åˆ)

**Week 3**:
- Day 1-3: ADMMConstraintHandler å®Ÿè£…
- Day 4-5: ADMMåæŸæ€§ãƒ†ã‚¹ãƒˆ

**Week 4**:
- Day 1-2: COMARewardEstimator å®Ÿè£…
- Day 3-4: HybridRLStrategy æ‹¡å¼µ
- Day 5: æ€§èƒ½æ¯”è¼ƒå®Ÿé¨“

**Milestone**: Phase 1Aã‚ˆã‚Š10%ä»¥ä¸Šã®æ€§èƒ½å‘ä¸Š

### Week 5-6: Phase 2 (ã‚¹ã‚±ãƒ¼ãƒ«å¯¾å¿œ)

**Week 5**:
- Day 1-2: GraphConverter å®Ÿè£…
- Day 3-4: EfficientPathGenerator å®Ÿè£…
- Day 5: ãƒ¡ãƒ¢ãƒªãƒ—ãƒ­ãƒ•ã‚¡ã‚¤ãƒªãƒ³ã‚°

**Week 6**:
- Day 1-3: å¤§è¦æ¨¡å®Ÿé¨“
- Day 4-5: çµæœåˆ†æ & ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆ

**Milestone**: 50ãƒãƒ¼ãƒ‰ä»¥ä¸Šã®å•é¡Œã§å®Ÿç”¨çš„ãªæ€§èƒ½

### Week 7-8: æœ€çµ‚èª¿æ•´ & è«–æ–‡æº–å‚™

- ãƒã‚¤ãƒ‘ãƒ¼ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ãƒãƒ¥ãƒ¼ãƒ‹ãƒ³ã‚°
- è¿½åŠ å®Ÿé¨“
- å¯è¦–åŒ–ãƒ»ã‚°ãƒ©ãƒ•ä½œæˆ
- ãƒ‰ã‚­ãƒ¥ãƒ¡ãƒ³ãƒˆæ•´å‚™

---

## âš™ï¸ è¨­å®šä¾‹

### Phase 1Aè¨­å®š

```json
{
  "expt_name": "hybrid_rl_phase1a",
  "training_strategy": "hybrid_rl",

  "model": {
    "num_layers": 3,
    "hidden_dim": 128,
    "dropout_rate": 0.3
  },

  "hybrid_rl": {
    "use_flow_allocation": true,
    "use_admm": false,
    "use_coma": false,

    "flow_rounder": {
      "type": "simple",
      "num_iterations": 2
    },

    "reward": {
      "alpha": 1.0,
      "beta": 0.1,
      "gamma": 5.0
    },

    "baseline_momentum": 0.9
  },

  "path_generator": {
    "k_paths": 4,
    "edge_disjoint": false
  },

  "training": {
    "learning_rate": 0.0001,
    "batch_size": 32,
    "max_epochs": 100
  }
}
```

### Phase 1Bè¨­å®š

```json
{
  "expt_name": "hybrid_rl_phase1b",
  "training_strategy": "hybrid_rl",

  "hybrid_rl": {
    "use_flow_allocation": true,
    "use_admm": true,
    "use_coma": true,

    "admm": {
      "rho": 1.0,
      "num_iterations": 5
    },

    "coma": {
      "num_samples": 5
    },

    "reward": {
      "alpha": 1.0,
      "beta": 0.2,
      "gamma": 5.0
    }
  },

  "training": {
    "learning_rate": 0.0001,
    "batch_size": 32,
    "max_epochs": 150
  }
}
```

---

## ğŸ”§ ãƒˆãƒ©ãƒ–ãƒ«ã‚·ãƒ¥ãƒ¼ãƒ†ã‚£ãƒ³ã‚°

### Issue 1: ADMMãŒåæŸã—ãªã„

**ç—‡çŠ¶**: ADMMåå¾©å¾Œã‚‚åˆ¶ç´„é•åãŒæ®‹ã‚‹

**è§£æ±ºç­–**:
1. `rho` ãƒ‘ãƒ©ãƒ¡ãƒ¼ã‚¿ã‚’èª¿æ•´ï¼ˆ0.1 â†’ 10.0ã®ç¯„å›²ï¼‰
2. åå¾©å›æ•°ã‚’å¢—ã‚„ã™ï¼ˆ5 â†’ 10ï¼‰
3. åˆæœŸãƒ•ãƒ­ãƒ¼ã®å“è³ªã‚’æ”¹å–„ï¼ˆGCNã®äº‹å‰è¨“ç·´ï¼‰

### Issue 2: COMAå ±é…¬æ¨å®šã®åˆ†æ•£ãŒå¤§ãã„

**ç—‡çŠ¶**: å­¦ç¿’ãŒä¸å®‰å®š

**è§£æ±ºç­–**:
1. ã‚µãƒ³ãƒ—ãƒ«æ•°ã‚’å¢—ã‚„ã™ï¼ˆ5 â†’ 10ï¼‰
2. ä»£æ›¿è¡Œå‹•ã®ç”Ÿæˆæ–¹æ³•ã‚’å¤‰æ›´ï¼ˆuniform â†’ Gaussian noiseï¼‰
3. Advantage normalizationã‚’è¿½åŠ 

### Issue 3: ãƒ¡ãƒ¢ãƒªä¸è¶³

**ç—‡çŠ¶**: OOM ã‚¨ãƒ©ãƒ¼

**è§£æ±ºç­–**:
1. ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’æ¸›ã‚‰ã™
2. Gradient accumulationã‚’ä½¿ç”¨
3. Phase 2ã®GraphConverterã§ç–è¡¨ç¾ã«å¤‰æ›

### Issue 4: æ—¢å­˜RL-GCNã‚ˆã‚Šæ€§èƒ½ãŒæ‚ªã„

**ç—‡çŠ¶**: Phase 1Aã§æœŸå¾…ã•ã‚Œã‚‹æ€§èƒ½ãŒå‡ºãªã„

**è¨ºæ–­**:
1. FlowAllocationLayerã®ã‚¹ã‚³ã‚¢é›†ç´„ãŒæ­£ã—ã„ã‹ç¢ºèª
2. å ±é…¬é–¢æ•°ã®å„é …ã®é‡ã¿ (Î±, Î², Î³) ã‚’èª¿æ•´
3. PathGeneratorãŒé©åˆ‡ãªãƒ‘ã‚¹ã‚’ç”Ÿæˆã—ã¦ã„ã‚‹ã‹ç¢ºèª
4. SimpleFlowRounderãŒéåº¦ã«åˆ¶ç´„ã‚’æº€ãŸãã†ã¨ã—ã¦ã„ãªã„ã‹

---

## ğŸ“š å‚è€ƒæ–‡çŒ®

1. **Teal**: Xu et al., "Teal: Learning-Accelerated Optimization of WAN Traffic Engineering", SIGCOMM 2023
2. **ADMM**: Boyd et al., "Distributed Optimization and Statistical Learning via ADMM", 2011
3. **COMA**: Foerster et al., "Counterfactual Multi-Agent Policy Gradients", AAAI 2018
4. **Residual Gated GCN**: Bresson & Laurent, "Residual Gated Graph ConvNets", 2017
5. **REINFORCE**: Williams, "Simple Statistical Gradient-Following Algorithms", 1992

---

## âœ… æ¬¡ã®ã‚¹ãƒ†ãƒƒãƒ—

**å³åº§ã«é–‹å§‹**:
1. âœ… è¨ˆç”»ãƒ¬ãƒãƒ¼ãƒˆä½œæˆå®Œäº†
2. ğŸš€ Phase 1Aå®Ÿè£…é–‹å§‹
   - FlowAllocationLayer ã‹ã‚‰å®Ÿè£…
   - æ®µéšçš„ãªãƒ†ã‚¹ãƒˆãƒ»æ¤œè¨¼

**çŸ­æœŸç›®æ¨™ï¼ˆ2é€±é–“ï¼‰**:
- Phase 1Aå®Œæˆ
- æ—¢å­˜RL-GCNã¨ã®æ€§èƒ½æ¯”è¼ƒ

**ä¸­æœŸç›®æ¨™ï¼ˆ1ãƒ¶æœˆï¼‰**:
- Phase 1Bå®Œæˆ
- ADMM/COMAåŠ¹æœã®å®Ÿè¨¼

**é•·æœŸç›®æ¨™ï¼ˆ2ãƒ¶æœˆï¼‰**:
- Phase 2å®Œæˆ
- å¤§è¦æ¨¡å•é¡Œã§ã®è©•ä¾¡
- è«–æ–‡åŸ·ç­†æº–å‚™

---

**ä½œæˆè€…**: Claude (Anthropic)
**ãƒãƒ¼ã‚¸ãƒ§ãƒ³**: 1.0
**æœ€çµ‚æ›´æ–°**: 2025-10-22
