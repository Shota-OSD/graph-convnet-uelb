# RL-GCN 学習分析レポート - Load Factor修正後
**日付**: 2025-10-19
**学習結果**: logs/training_results_20251019_200449.txt
**状態**: 致命的なバグを修正、学習は機能しているが不安定

---

## エグゼクティブサマリー

**Load Factor計算の致命的なバグを修正**し、初めて正しい報酬計算が可能になりました。現在の学習状況:
- ✅ Load Factorが有限値（0.33-0.37）に（以前は全て`inf`）
- ✅ 複数のエポックで正の報酬を獲得（最大+0.81）
- ✅ Finite Solution Rate 100%（以前は60%）
- ⚠️ 学習は不安定で報酬が振動
- ⚠️ Entropyが0.5787で固定（探索不足）
- ⚠️ パスの15-21%が不完全

---

## 1. 致命的なバグ修正 - Load Factor計算

### 発見された問題
**ファイル**: `src/common/graph/graph_utils.py:166-167`

**問題点**: `torch.where()`でスカラーテンソルを使用したため、ブロードキャストエラーが発生:
```python
# 誤り - スカラーテンソルを生成
load_factors = torch.where(
    edges_capacity == 0,
    torch.where(
        bs_edges_summed == 0,
        torch.tensor(0.0, device=device),          # ❌ スカラー
        torch.tensor(float('inf'), device=device)  # ❌ スカラー
    ),
    bs_edges_summed.float() / edges_capacity.float()
)
```

**結果**: 全てのLoad Factorが`inf`になり、全ての報酬が-5.0（無効エッジペナルティ）になっていた

### 適用した修正
```python
# 正解 - 正しい形状のテンソルを生成
load_factors = torch.where(
    edges_capacity == 0,
    torch.where(
        bs_edges_summed == 0,
        torch.zeros_like(bs_edges_summed, dtype=torch.float),  # ✅ 正しい形状
        torch.full_like(bs_edges_summed, float('inf'), dtype=torch.float)  # ✅ 正しい形状
    ),
    bs_edges_summed.float() / edges_capacity.float()
)
```

### 影響
| 指標 | 修正前 | 修正後 | 改善 |
|------|--------|--------|------|
| Load Factor | inf (100%) | 0.33-0.37 | ✅ 有限値 |
| Finite Solution Rate | 59.2% | 100.0% | +40.8% |
| 報酬範囲 | -70 ~ -81 | -1.5 ~ +0.81 | ✅ 正の報酬 |
| Avg Finite LF | 0.36 | 0.35 | 一貫性 |

---

## 2. 現在の学習パフォーマンス

### 学習結果（10エポック）

#### 報酬の推移
```
Epoch  Reward     Advantage  Complete%  Finite%  Load Factor
─────────────────────────────────────────────────────────────
1      -1.13      +1.37      79.6%      100%     0.3524
2      -1.49      -0.35      79.2%      100%     0.3385
3      +0.17      +1.25      84.2%      100%     0.3460
4      +0.81      +1.57      85.0%      100%     0.3684  ← 最良
5      +0.07      -0.34      82.2%      100%     0.3346
6      -0.79      -1.14      81.6%      100%     0.3542
7      -0.54      -0.54      81.4%      100%     0.3472
8      -0.08      -0.87      83.6%      100%     0.3713
9      -0.48      -1.03      82.2%      100%     0.3623
10     -0.47      -0.71      81.2%      100%     0.3550
```

#### 主要な観察

**ポジティブな兆候**:
1. **Baseline値の変化**: -2.26 → +0.21（エポック1→10）
   - モデルが徐々に良い報酬を得られるようになっていることを示す
2. **報酬のピーク**: エポック3, 4, 5, 8で正の報酬を達成
3. **有限解**: 全エポックで100%（完璧）
4. **容量違反**: 0-1%（優れた制約充足）

**懸念される兆候**:
1. **報酬の振動**: +0.81 → -0.79（エポック4→6）
2. **不完全なパス**: 15-21%が目的地に到達しない
3. **Entropy固定**: 0.5787のまま（探索の変化なし）

---

## 3. 問題分析

### 問題1: 報酬の振動と不安定性

**症状**:
- 報酬が変動: +0.81（エポック4）→ +0.07（エポック5）→ -0.79（エポック6）
- Advantageが正と負を交互に繰り返す
- エポック4以降、明確な改善傾向が見られない

**根本原因**:

#### A. 不完全パスペナルティが支配的
- パスの15-21%が不完全 → サンプルあたり-5.0または-10.0のペナルティ
- バッチサイズ20の場合: 3-4サンプルが厳しいペナルティを受ける
- エポック4の計算例:
  ```
  完全なパス（85%）: 17サンプル × ~9.0報酬 = +153
  不完全（15%）:      3サンプル × -5.0ペナルティ = -15
  平均: (+153 - 15) / 20 = サンプルあたり+6.9

  しかし実際の報告値: +0.81 → Advantage正規化が大きさを減少
  ```

#### B. Advantage正規化が信号を減少
`reinforcement_strategy.py:350-359`より:
```python
if self.normalize_advantages and self.batch_size > 1:
    advantages = (advantages - advantage_mean) / (advantage_std + 1e-8)
```
- Advantageを平均=0, 標準偏差=1に正規化
- **問題**: 勾配信号の大きさを減少させる
- 小さいバッチ（20サンプル）+ 正規化 = 高分散

#### C. Baseline Momentumが高すぎる（0.95）
```python
self.reward_baseline = (
    0.95 * self.reward_baseline +
    0.05 * batch_mean_reward
)
```
- 現在のバッチ報酬の5%のみがBaselineを更新
- **結果**: Baselineが実際のパフォーマンスに遅れる
- パフォーマンスが急速に改善する時（エポック3→4）、Baselineは過小評価
- パフォーマンスが低下する時（エポック4→6）、Baselineは過大評価
- これがAdvantageの振動を生む

### 問題2: Entropyが0.5787で固定

**症状**:
- 全エポックでEntropy = 0.5787
- 探索-活用のトレードオフが見えない
- モデル出力分布が変化していない

**根本原因**:

#### A. Temperature Schedulingが見えない
温度値は計算されているが、出力に表示されていない:
- 期待値: 温度 1.0 → 0.5（線形減衰、エポックあたり0.05）
- 実際の表示: ログに欠落

温度が実際に適用されているか確認:
```python
# reinforcement_strategy.py:122
current_temp = self.get_current_temperature()  # スケジュールされた値を返す

# reinforcement_strategy.py:129
temperature=current_temp,  # PathSamplerに渡される
```

**仮説**: 温度スケジューリングは実装されているが:
1. Entropy計算は`y_preds`（モデル出力）を使用、PathSampler確率ではない
2. モデル出力分布（温度スケーリング前）があまり変化していない
3. 温度がサンプリングに影響するか、log_probsのみに影響するか検証が必要

#### B. モデルがエッジ確率を学習していない
`reinforcement_strategy.py:377-379`のEntropy:
```python
probs = F.softmax(y_preds, dim=-1)
log_probs_for_entropy = F.log_softmax(y_preds, dim=-1)
entropy = -(probs * log_probs_for_entropy).sum(dim=-1).mean()
```

これは**モデル出力**（`y_preds`）のEntropyを測定、PathSampler分布ではない。

**意味**: モデルのエッジ予測確率がエポック間で大きく変化していない。

### 問題3: 不完全なパス（15-21%）

**症状**:
- Complete Paths Rate: 79-85%（15-21%が不完全）
- これらのサンプルは-5.0または-10.0のペナルティを受ける
- 平均報酬に深刻な影響

**根本原因**:

#### A. スパースなグラフトポロジー
PathSamplerのデバッグ出力より:
```
有効エッジ数: 30/196（エッジ密度15%）
```
- 14ノードグラフで有効エッジが30個のみ
- 一部のノードペアにはパスが存在しない可能性（グラフが非連結）
- またはパスが長すぎて貪欲サンプリングでは見つ���られない

#### B. Top-pサンプリングの探索vs活用
現在の設定: `top_p = 0.9`
- 上位90%の確率質量からサンプリング
- 必要な低確率エッジを見逃す可能性
- 特に学習初期、モデルが未訓練の時に問題

#### C. 最大ステップ数の制限
`path_sampler.py:226`より:
```python
max_steps = self.num_nodes * 2  # 14 * 2 = 28ステップ
```
- 目的地到達まで最大28ステップ許可
- 平均パス長: 6.2-6.4エッジ
- したがって制限は問題ではない

**最も可能性が高い**: モデルがまだ正しいエッジを予測することを学習していないため、行き止まりになる。

---

## 4. 提案する解決策

### 解決策1: 報酬学習の安定化

#### A. Baseline Momentumを減少
**ファイル**: `configs/gcn/rl_min_unconstrained.json`

**現在**:
```json
"rl_baseline_momentum": 0.95
```

**提案**:
```json
"rl_baseline_momentum": 0.90
```

**理由**:
- 変化する報酬分布への迅速な適応
- Advantageの振動を減少
- 更新率10% vs 現在の5%

#### B. Advantage正規化を無効化または減少
**ファイル**: `configs/gcn/rl_min_unconstrained.json`

**オプション1 - 無効化**（小バッチに推奨）:
```json
"rl_normalize_advantages": false
```

**オプション2 - バッチサイズを増加**:
```json
"batch_size": 50  // 現在20
```

**理由**:
- バッチサイズ20は安定した正規化統計には小さすぎる
- 正規化が不必要に勾配の大きさを減少
- BaselineありのREINFORCEは既に分散を減少させている

#### C. 不完全パスの報酬スケールを調整
**ファイル**: `src/gcn/training/reinforcement_strategy.py:300-308`

**現在**:
```python
if num_incomplete <= 1:
    reward_i = -5.0
else:
    reward_i = -10.0
```

**提案**（段階的ペナルティ）:
```python
if num_incomplete <= 1:
    reward_i = -2.0  # 1つの不完全パス
elif num_incomplete <= 2:
    reward_i = -5.0  # 2つの不完全パス
else:
    reward_i = -10.0  # 複数の不完全パス
```

**理由**:
- 現在のペナルティ（-5.0）が最良報酬（~9.0）に対して厳しすぎる
- ペナルティ範囲: -5.0 ~ -10.0（スパン = 5）
- 有効時の報酬範囲: 7.0 ~ 10.0（スパン = 3）
- **問題**: ペナルティが勾配信号を支配
- より柔軟なペナルティでモデルが部分的な進歩から学習可能

### 解決策2: 探索の改善（Entropy修正）

#### A. 温度が適用されているか検証
**アクション**: 明示的な温度ログを追加

**ファイル**: `src/gcn/train/trainer.py`（485行付近）

**追加**:
```python
temp_str = f", Temp: {rl_metrics['temperature']:.2f}" if 'temperature' in rl_metrics else ""
epoch_bar.write(f"  RL Metrics - Reward: {rl_metrics['reward']:.4f}, "
                f"Advantage: {rl_metrics.get('advantage', 0):.4f}, "
                f"Entropy: {rl_metrics.get('entropy', 0):.4f}{temp_str}")
```

**確認**: 温度が10エポックで1.0 → 0.5に減少するか検証

#### B. Entropy Weightを増加
**ファイル**: `configs/gcn/rl_min_unconstrained.json`

**現在**:
```json
"rl_entropy_weight": 0.01
```

**提案**:
```json
"rl_entropy_weight": 0.05
```

**理由**:
- より強い探索のインセンティブ
- モデルが異なるエッジを試すよう促す
- 不完全なコモディティのパス発見に役立つ可能性

#### C. サンプリング分布からEntropyを計算
**ファイル**: `src/gcn/training/reinforcement_strategy.py`

**現在**（377-379行）:
```python
# モデル出力y_predsを使用
probs = F.softmax(y_preds, dim=-1)
entropy = -(probs * log_probs_for_entropy).sum(dim=-1).mean()
```

**提案**: PathSamplerのエッジ確率から計算
- 実際のサンプリング分布（温度スケーリング後）を反映
- 探索のより意味のある指標

**実装**: PathSamplerがエッジ確率統計を返す必要がある

### 解決策3: 不完全パスの削減

#### A. 教師あり事前学習でウォームスタート
**戦略**: まず真値の最短パスでモデルを訓練

**ステップ**:
1. 訓練データの最短パスラベルを生成
2. 10-20エポック、交差エントロピー損失で訓練
3. REINFORCEでファインチューニング

**利点**:
- モデルが有効なグラフ構造を学習
- 不完全パスを15-21%から<5%に削減
- RLのより良い初期化を提供

#### B. カリキュラム学習
**戦略**: より簡単な（小さい、密な）グラフから開始

**実装**:
```python
# 10ノードグラフから開始（エポック1-20）
# 12ノードグラフに移行（エポック21-40）
# 完全な14ノードグラフ（エポック41+）
```

**利点**:
- モデルがシンプルなトポロジーでパス発見を学習
- 徐々に難易度を上げる
- 不完全パス率を削減

#### C. Top-pサンプリングを調整
**ファイル**: `configs/gcn/rl_min_unconstrained.json`

**現在**:
```json
"rl_sampling_top_p": 0.9
```

**提案**（初期探索）:
```json
"rl_sampling_top_p": 0.95
```

**理由**:
- 上位95% vs 90%からサンプリング可能
- より多くのエッジ候補を含む
- スパースグラフでパス発見に役立つ可能性

### 解決策4: 訓練期間の延長

#### A. より多くのエポック
**現在**: 10エポック
**提案**: 50-100エポック

**理由**:
- エポック4が最良パフォーマンス（+0.81報酬）を示した
- 安定化と改善にはより多くの時間が必要
- REINFORCEはサンプル非効率、多くの反復が必要

#### B. 学習率スケジュール
**ファイル**: `configs/gcn/rl_min_unconstrained.json`

**追加**:
```json
"lr_decay_rate": 0.95,
"lr_decay_step": 10
```

**理由**:
- 高いLRで開始し高速な初期学習
- 徐々に減衰してファインチューニングと安定化
- 後のエポックでの振動を削減

---

## 5. 推奨アクションプラン

### フェーズ1: 即時修正（高優先度）

1. **温度ログ追加**（5分）
   - 温度スケジューリングが機能しているか検証
   - エポック進捗バー出力に追加

2. **Baseline Momentumを削減**（2分）
   - 0.95から0.90に変更
   - 報酬変化への迅速な適応

3. **Advantage正規化を無効化**（2分）
   - batch_size=20では分散が大きすぎる
   - Baselineに分散削減を任せる

4. **不完全パスペナルティを緩和**（5分）
   - 1つ不完全: -5.0 → -2.0
   - 部分的な進歩からの学習を可能に

5. **50エポック実行**（訓練時間15分）
   - 現在の10エポックはREINFORCEには短すぎる
   - 改善傾向を監視

**期待される改善**:
- 報酬の振動が削減
- 50エポックにわたる徐々な改善傾向
- Complete paths rateが85-90%に増加

### フェーズ2: 探索強化（中優先度）

6. **Entropy Weightを増加**（2分）
   - 0.01 → 0.05
   - エッジの多様性を促進

7. **Top-pを調整**（2分）
   - 0.9 → 0.95
   - 初期により多くの探索を許可

8. **温度効果を検証**（分析10分）
   - 温度を手動設定した時にEntropyが変化するか確認
   - Entropy計算方法の修正が必要かも

**期待される改善**:
- Entropyが変動し始める（0.58 → 0.50-0.60範囲）
- モデルが異なるエッジ組み合わせを探索
- 以前は不完全だったコモディティのパスを発見

### フェーズ3: 長期改善（低優先度）

9. **教師あり事前学習**（実装1時間）
   - 最短パスラベルを生成
   - 交差エントロピーでモデルを事前訓練
   - 不完全パスを<5%に削減

10. **カリキュラム学習**（実装2時間）
    - 段階的なグラフサイズ増加を実装
    - データセット修正が必要

11. **���ッチサイズを増加**（メモリが許せば）
    - 20 → 50または100
    - Advantage正規化を有効化
    - 勾配分散を削減

---

## 6. 成功指標

### 短期（フェーズ1後 - 50エポック）
| 指標 | 現在 | 目標 | 測定方法 |
|------|------|------|----------|
| 報酬（平均） | -0.47 ~ +0.81 | +2.0 ~ +5.0 | 安定した正の値 |
| 報酬（傾向） | 振動 | 増加 | 線形回帰の傾き > 0 |
| Complete Paths | 79-85% | 85-90% | 直近10エポック平均 |
| Advantage標準偏差 | 高 | <2.0 | 振動の削減 |
| Load Factor | 0.33-0.37 | 0.25-0.30 | より良い解 |

### 中期（フェーズ2後）
| 指標 | 目標 | 測定方法 |
|------|------|----------|
| Entropy | 0.55-0.60（変動） | 探索を示す |
| Complete Paths | 90-95% | エポック間で一貫 |
| Capacity Violation | <1% | 維持 |

### 長期（フェーズ3後）
| 指標 | 目標 | 測定方法 |
|------|------|----------|
| 報酬 | +7.0 ~ +9.0 | ほぼ最適 |
| Complete Paths | >95% | 失敗はまれ |
| Load Factor | <0.20 | ビームサーチと競合 |

---

## 7. コード変更サマリー

### バグ修正で変更したファイル

1. **`src/common/graph/graph_utils.py:166-167`**
   - 修正: `torch.tensor()` → `torch.zeros_like()` / `torch.full_like()`
   - 影響: Load factorが有限値に

2. **`src/gcn/algorithms/path_sampler.py:73, 258-262`**
   - 修正: 容量ルックアップのためのバッチインデックス追跡
   - 影響: フォールバックモードでの正しい容量チェック

### 解決策のために変更するファイル

1. **`configs/gcn/rl_min_unconstrained.json`**
   - Baseline momentum: 0.95 → 0.90
   - Normalize advantages: true → false
   - Entropy weight: 0.01 → 0.05
   - Top-p: 0.9 → 0.95
   - Epochs: 10 → 50

2. **`src/gcn/training/reinforcement_strategy.py:300-308`**
   - 不完全パスペナルティ: -5.0/-10.0 → -2.0/-5.0/-10.0（段階的）

3. **`src/gcn/train/trainer.py:485`**（既に実装済み、出力を検証）
   - 進捗バー表示に温度を追加

---

## 8. 結論

**状態**: ✅ **致命的なバグを修正 - 学習が機能しています**

**現状**:
- Load Factor計算が正しく動作（有限値）
- モデルが一部のエポックで正の報酬を達成
- Finite solution rate 100%
- 学習は不安定だが可能性を示している

**次のステップ**:
1. フェーズ1の修正を適用（baseline momentum、正規化、ペナルティ）
2. 50エポック実行して学習傾向を観察
3. 結果を分析し、必要に応じてフェーズ2に進む

**予後**:
致命的なバグが修正され、フェーズ1の調整を適用すると、以下が期待されます:
- エポック20-30までに安定した正の報酬
- エポック50までにComplete paths rate >90%
- ビームサーチと競合するパフォーマンス（load factor <0.25）

基盤は今や堅固です - 残りの問題はハイパーパラメータ調整と訓練期間であり、根本的なアルゴリズムの問題ではありません。

---

**レポート作成日**: 2025-10-19
**次回レビュー**: フェーズ1修正適用後の50エポック訓練実行後
