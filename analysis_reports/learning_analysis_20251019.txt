================================================================================
RL-GCN学習状況分析レポート
================================================================================
日時: 2025-10-19
設定: configs/gcn/rl_min_unconstrained.json
エポック数: 10
バッチサイズ: 20
学習率: 0.001 (decay: 1.5 every 3 epochs)

================================================================================
1. 総合評価
================================================================================

【判定】学習は進行しているが、複数の問題が存在する（評価：C+）

【良好な点】✓
  • パス完成率が高い (平均80.3%)
  • 有限解の割合が適度 (平均59.2%)
  • 容量違反が非常に少ない (平均0.79%)
  • 学習が安定化している (標準偏差が減少)
  • 有限解の負荷率は良好 (0.31-0.40)

【問題点】✗
  • 報酬が改善していない (初期: -70.1 → 最終: -76.1)
  • アドバンテージがほぼ常に負 (10エポック中9エポック)
  • エントロピーが完全に一定 (0.5787で固定)
  • 約40%のサンプルが依然として無効なパス(inf)を含む

================================================================================
2. 主要メトリクスの詳細分析
================================================================================

2.1 報酬トレンド
  初期報酬:    -70.10
  最終報酬:    -76.08
  最良報酬:    -70.10 (Epoch 0)
  最悪報酬:    -81.08 (Epoch 5)
  トレンド:    -0.079/epoch (悪化)

  【解釈】
  報酬が改善していないが、これは以下の要因による可能性:
    1. 報酬が常に負（ペナルティベース）で改善シグナルが弱い
    2. 約40%のサンプルが-100ペナルティを受けている
    3. ベースラインが報酬に追いついていない

2.2 アドバンテージ分析
  正のアドバンテージ: 1/10 epochs (10%)
  トレンド: +0.310/epoch (改善傾向)

  【解釈】
  アドバンテージがほぼ常に負 = サンプルがベースラインより悪い
  これは学習シグナルが弱く、ポリシー更新が効果的でない可能性を示唆

2.3 エントロピー分析
  平均: 0.5787
  標準偏差: 0.00002
  変動範囲: 0.000061

  【解釈】
  エントロピーが完全に固定 = 探索が変化していない
  原因:
    - 温度パラメータ固定 (temperature=1.0)
    - モデル出力が変化していない
    - top-pサンプリング(0.9)が常に同じ分布を生成

2.4 パス品質分析
  完成率:     80.3% (良好)
  有限解率:   59.2% (中程度)
  平均長:     6.39エッジ (14ノードグラフで適切)
  負荷率:     0.36 (有限解のみ、良好)

  【解釈】
  パスの品質自体は悪くない
  しかし、約40%が無効なパス（inf負荷率）を含む

================================================================================
3. 根本原因の分析
================================================================================

3.1 なぜ報酬が改善しないのか？

【原因1】報酬設計の問題
  • 現在の報酬: 常に負の値（-0.3～-100）
  • 有限解: -0.3～-0.4（負荷率の負値）
  • 無効解: -100（厳しいペナルティ）
  
  問題点:
    - 報酬の範囲が広すぎる（-0.3と-100の差が大きい）
    - 正の報酬要素がない
    - 段階的な改善を促すシグナルが弱い

【原因2】アドバンテージの問題
  • ベースラインモメンタム: 0.9
  • これは比較的速い更新を意味する
  • サンプリングの分散が大きいため、ベースラインが実際の報酬に追いつけない

【原因3】On-policy学習の限界
  • REINFORCEはon-policy（現在のポリシーからサンプル）
  • ポリシーが改善する前にベースラインが更新される
  • 結果: アドバンテージが常に負になる

3.2 なぜエントロピーが固定なのか？

【原因1】温度パラメータが固定
  • temperature = 1.0（固定）
  • エントロピーボーナス = 0.1（固定）
  • モデルが学習しても、サンプリング温度が変わらない

【原因2】モデルの出力が変化していない可能性
  • エントロピーが0.5787で完全に固定
  • これはモデルの出力確率分布が全く変わっていないことを示唆
  • 学習率が低すぎるか、勾配が消失している可能性

3.3 なぜ40%が無効なパスなのか？

【原因】不完全なパス or 無効なエッジ使用
  • 不完全なパス: 目的地に到達できない → -100ペナルティ
  • 無効なエッジ: 容量0のエッジを使用 → -100ペナルティ
  
  これらは以下を示唆:
    1. モデルがまだ有効なパスを一貫して生成できていない
    2. グラフが疎（30/196エッジのみ有効）で到達困難
    3. ペナルティが厳しすぎて学習シグナルが弱い

================================================================================
4. 推奨される改善策
================================================================================

【優先度：高】即座に実施すべき

1. 報酬設計の改善
   現在: reward = -load_factor (無効なら-100)
   改善案:
   ```python
   if all_paths_complete and not np.isinf(load_factor):
       # 有効なソリューション
       if load_factor <= 1.0:
           reward = 100 - load_factor * 10  # 90～100の正の報酬
       else:
           reward = 50 - load_factor * 5    # 容量超過でもある程度の報酬
   elif all_paths_complete:
       # パスは完成したが無効なエッジ使用
       reward = -10
   else:
       # パス不完全
       reward = -50
   ```
   
   効果:
     - 正の報酬を導入 → 改善シグナルが明確に
     - ペナルティの段階を細分化 → 学習が進みやすい

2. ベースラインモメンタムの調整
   現在: 0.9
   推奨: 0.95 or 0.99
   
   効果: ベースラインの更新を遅くし、アドバンテージが正になりやすくする

3. 温度スケジューリングの導入
   ```python
   temperature = max(0.5, 1.0 - epoch * 0.05)  # 1.0 → 0.5へ減少
   ```
   
   効果: 初期は探索、後期は活用へシフト

【優先度：中】効果が期待できる

4. 学習率の調整
   現在: 0.001 (decay 1.5 every 3 epochs)
   推奨: 0.0001 or より緩やかなdecay
   
   理由: エントロピー固定は学習が進んでいない可能性を示唆

5. バッチサイズの増加
   現在: 20
   推奨: 50 or 100
   
   効果: 勾配推定の分散を減らす

6. より長いトレーニング
   現在: 10 epochs
   推奨: 50-100 epochs
   
   理由: RL学習は一般的に時間がかかる

【優先度：低】長期的な改善

7. Shaped Reward の導入
   - パス長に応じたボーナス
   - 中間目標への報酬
   - 探索ボーナス

8. より高度なRL手法
   - PPO (Proximal Policy Optimization)
   - A2C/A3C (Advantage Actor-Critic)
   
   理由: より安定した学習が期待できる

================================================================================
5. 結論
================================================================================

【現状】
学習は進行しているが、以下の理由で効果的ではない:
  1. 報酬が常に負でシグナルが弱い
  2. アドバンテージがほぼ常に負で更新が非効率
  3. エントロピー固定で探索が変化していない
  4. 40%のサンプルが無効で学習に寄与していない

【次のステップ】
優先度高の3つの改善を実施することを強く推奨:
  1. 報酬設計の改善（正の報酬導入）
  2. ベースラインモメンタム調整（0.9 → 0.95）
  3. 温度スケジューリング導入

これらの変更により、学習の効率が大幅に改善することが期待される。

================================================================================
