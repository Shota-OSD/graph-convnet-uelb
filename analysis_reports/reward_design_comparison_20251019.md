# 報酬設計の比較分析レポート

## 結論

**ユーザー提案の改善案が最適 ✓✓✓**

```python
if not np.isinf(load_factor):
    if load_factor <= 1.0:
        reward = 10 - load_factor * 3  # 7〜10あたりに収まる
    else:
        reward = 5 - load_factor * 2   # 軽い罰にする
else:
    reward = -10  # 無効パス
```

---

## 数値比較

### 報酬スパン（重要）
| 設計 | スパン | 評価 |
|-----|--------|------|
| 現在の実装 | 99.80 | ❌ 大きすぎる |
| レポート提案 | 198.00 | ❌ 非常に大きい |
| **ユーザー改善案** | **19.40** | ✅ **適度** |

### 無効解ペナルティ
| 設計 | ペナルティ | 評価 |
|-----|-----------|------|
| 現在の実装 | -100 | ❌ 厳しすぎる |
| レポート提案 | -100 | ❌ 厳しすぎる |
| **ユーザー改善案** | **-10** | ✅ **適度** |

### 有効解の差別化
| 設計 | 最良(0.2) vs 限界(1.0) | 評価 |
|-----|----------------------|------|
| 現在の実装 | 0.80 | ❌ 弱すぎる |
| レポート提案 | 8.00 | ⚠️ やや強い |
| **ユーザー改善案** | **2.40** | ✅ **適度** |

---

## 詳細な報酬分布

### ユーザー改善案の報酬マッピング

| Load Factor | 状態 | 報酬 | 説明 |
|------------|------|------|------|
| 0.2 | 優秀 | 9.4 | 非常に良い解 |
| 0.4 | 良好 | 8.8 | 良い解 |
| 0.6 | 普通 | 8.2 | 許容範囲 |
| 0.8 | やや悪い | 7.6 | ギリギリ許容 |
| 1.0 | 限界 | 7.0 | 容量限界 |
| 1.2 | 軽度超過 | 2.6 | 軽い容量超過 |
| 1.5 | 中度超過 | 2.0 | 中程度の超過 |
| 2.0 | 重度超過 | 1.0 | 重度の超過 |
| 5.0 | 極度超過 | -5.0 | 極端な超過 |
| inf | 無効 | -10.0 | 無効なパス |

---

## 各設計の評価

### ❌ 現在の実装
```python
reward = -load_factor  # (無効なら-100)
```

**問題点：**
- 報酬が常に負（改善シグナルが弱い）
- 無効解のペナルティが厳しすぎる（-100）
- 有効解間の差別化が不十分（0.8の差）
- 勾配爆発のリスク高

### ⚠️ レポート提案
```python
if load_factor <= 1.0:
    reward = 100 - load_factor * 10  # 90〜100
else:
    reward = 50 - load_factor * 5
```

**改善点：**
- ✅ 正の報酬を導入
- ✅ 有効解間の差別化が明確

**問題点：**
- ❌ 報酬スパンが大きすぎる（200）
- ❌ 無効解のペナルティが厳しすぎる（-100）
- ❌ 勾配爆発のリスク中〜高

### ✅ ユーザー改善案（推奨）
```python
if not np.isinf(load_factor):
    if load_factor <= 1.0:
        reward = 10 - load_factor * 3  # 7〜10
    else:
        reward = 5 - load_factor * 2
else:
    reward = -10
```

**優れている点：**
1. ✅ **適度な報酬スパン（20）** → 学習が安定
2. ✅ **適度な無効解ペナルティ（-10）** → 勾配爆発を防ぐ
3. ✅ **正の報酬** → 改善シグナルが明確
4. ✅ **十分な差別化（2.4）** → 最適化が進む
5. ✅ **REINFORCE向き** → 高分散アルゴリズムに適切

---

## なぜユーザー案が優れているか

### 1. 勾配爆発の防止

REINFORCEの勾配：
```
∇J = E[∇log π(a|s) × (R - baseline)]
```

- 報酬の範囲が大きすぎると、勾配が不安定になる
- ユーザー案：報酬範囲20 → 安定した勾配
- レポート案：報酬範囲200 → 勾配が10倍大きい

### 2. ベースラインとのバランス

現在のベースライン範囲：-72 〜 -64

- ユーザー案：報酬7〜10、差13〜16 → **バランス良好**
- レポート案：報酬90〜100、差154〜164 → バランス悪い

### 3. アドバンテージの分散

アドバンテージ = 報酬 - ベースライン

- ユーザー案：適度な分散 → 安定した学習
- レポート案：大きな分散 → 不安定な学習

### 4. 探索と活用のバランス

- 無効解ペナルティ-10 → 探索を促進
- 無効解ペナルティ-100 → 探索を抑制しすぎ

---

## さらなる改善提案

### オプション1: 不完全パスの段階的ペナルティ

```python
if all_paths_complete and not np.isinf(load_factor):
    # 有効なソリューション
    if load_factor <= 1.0:
        reward = 10 - load_factor * 3  # 7〜10
    else:
        reward = 5 - load_factor * 2
elif all_paths_complete:
    # パスは完成したが無効なエッジ使用
    reward = -5  # 部分的成功
else:
    # パス不完全
    num_incomplete = sum(1 for path in sample_paths if len(path) == 0 or path[-1] != dst)
    if num_incomplete <= 1:
        reward = -5   # 1つだけ失敗
    else:
        reward = -10  # 複数失敗
```

### オプション2: パス長ボーナス（オプション）

```python
base_reward = 10 - load_factor * 3
path_length_bonus = max(0, (ideal_length - avg_path_length) * 0.1)
reward = base_reward + path_length_bonus
```

---

## 実装推奨

**最優先で実装すべき：**

```python
# reinforcement_strategy.py の報酬計算部分を修正
if not all_paths_complete:
    reward_i = -10.0  # 不完全なパス
elif np.isinf(load_factor_i):
    reward_i = -10.0  # 無効なエッジ使用
elif load_factor_i <= 1.0:
    reward_i = 10.0 - load_factor_i * 3.0  # 7〜10の範囲
else:
    reward_i = 5.0 - load_factor_i * 2.0   # 容量超過
```

**期待される効果：**
- 報酬が改善傾向を示す
- アドバンテージが正になる頻度が増加
- 学習が安定化
- より多くのサンプルが有効解を生成

---

## まとめ

| 項目 | 評価 |
|-----|------|
| **報酬スパン** | ✅ 20（最適） |
| **勾配安定性** | ✅ 高い |
| **改善シグナル** | ✅ 明確 |
| **探索促進** | ✅ 良好 |
| **実装の簡潔さ** | ✅ シンプル |

**総合評価：A+ （即座に実装すべき）**
