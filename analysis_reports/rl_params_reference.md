## RL ハイパーパラメータ早見表（baseline・entropy・temperature）

本ドキュメントは `src/gcn/training/reinforcement_strategy.py` の設定キーのうち、探索・安定化に関わる主要パラメータの役割・効果・デフォルト・調整目安を簡潔にまとめたものです。

### rl_baseline_momentum
- **役割**: REINFORCE の移動平均ベースラインの慣性（分散低減のためのベースライン更新の平滑化）。
- **効果**: 値が高いほど報酬変動への追従は遅いが、勾配分散が小さくなり学習が安定しやすい。
- **デフォルト（コード）**: 0.9
- **調整の目安**: 0.85–0.95。学習が不安定→上げる、環境変化に鈍い→下げる。

### rl_entropy_weight
- **役割**: エントロピー正則化の係数（探索を促進）。
- **効果**: 値が高いほど行動分布が広がり探索が増える一方、収束が遅く/不安定になる場合がある。
- **デフォルト（コード）**: 0.01
- **調整の目安**: 0.01–0.1 程度。探索不足→上げる、収束が遅い/不安定→下げる。

### rl_use_trajectory_entropy
- **役割**: ステップ単位の分布エントロピーではなく、軌跡（経路）ベースのエントロピーを用いるかどうか。
- **効果**: 経路全体の多様性を促進。系列依存の探索を強めたい場合に有効。
- **デフォルト（コード）**: True

### rl_entropy_epsilon
- **役割**: 有効手への一様混合（ε-グリーディ風）の混合率。
- **効果**: ログitsが鋭いときでも一定の探索を担保し、極端な決め打ちを緩和。
- **デフォルト（コード）**: 0.05
- **調整の目安**: 0.01–0.1。決め打ちが強い→少し上げる。

### rl_sampling_temperature
- **役割**: サンプリング温度（低いほど決定的・高いほど多様）。初期温度としても用いられる。
- **効果**: 温度↑で探索↑、温度↓で利用（exploitation）↑。
- **デフォルト（コード）**: 1.0
- **調整の目安**: 0.8–2.0。探索初期は高め→学習とともに下げていくのが定石。

### rl_min_temperature
- **役割**: 温度スケジューリングの下限値。
- **効果**: 過度な決定性を防ぎ、最終段階でも少量の探索を残す。
- **デフォルト（コード）**: 0.5
- **調整の目安**: 0.5–0.8。最終段階でも探索を残したいなら高め。

### rl_temperature_decay
- **役割**: エポックごとの温度減衰量（スケジューリング速度）。
- **効果**: 大きいほど探索→利用への切り替えが速い（早く決定的に）。
- **デフォルト（コード）**: 0.05
- **調整の目安**: 0.02–0.05。データが少ない/不安定→小さめ、早期収束狙い→大きめ。

## 運用上の注意
- これらの値は主に `src/gcn/training/reinforcement_strategy.py` で `config.get(...)` により読み込まれます。
- 温度関連は `rl_use_sampling: true` のときに有効です。`rl_sampling_temperature` は初期温度としても参照され、`rl_min_temperature`・`rl_temperature_decay` と併用してエポック進行に応じて低下させます（詳細なスケジュールは実装を参照）。

## 現行設定例（探索強め・安定性寄り）
以下は `configs/gcn/rl_min_unconstrained.json` の例です。

```json
{
  "rl_baseline_momentum": 0.95,
  "rl_entropy_weight": 0.1,
  "rl_use_trajectory_entropy": true,
  "rl_entropy_epsilon": 0.05,

  "rl_use_sampling": true,
  "rl_sampling_temperature": 1.5,
  "rl_min_temperature": 0.8,
  "rl_temperature_decay": 0.035
}
```


