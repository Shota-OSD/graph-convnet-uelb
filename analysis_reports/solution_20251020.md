🥇 対処①：「行動可能ノード」だけでエントロピーを平均

🎯 目的

次ノードの選択肢が1つしかないノード（＝自明な行動）をエントロピー平均に含めないことで、
本当に探索余地のあるノードのみで多様性を評価。

💡 実装例
# y_preds: [B, N_from, N_to, C]
probs = F.softmax(y_preds / current_temp, dim=2)
log_probs = F.log_softmax(y_preds / current_temp, dim=2)

# 「2つ以上の行動可能ノード」がある箇所だけを対象
multi_choice = (probs > 1e-8).sum(dim=2) >= 2  # [B, N_from, C]

row_entropy = -(probs * log_probs).sum(dim=2)  # [B, N_from, C]
entropy = row_entropy[multi_choice].mean() if multi_choice.any() else row_entropy.mean()
✅ 効果
	•	固定値（0.0005など）から脱却し、0.05〜0.3程度で安定。
	•	探索可能なノードでのみボーナスが効く。

⸻

🥈 対処②：「サンプリング軌跡上」でエントロピーを算出

🎯 目的

全ノードの分布ではなく、**実際に行動したノード列（軌跡）**に基づいてエントロピーを計算。
これにより、学習に寄与する分布だけを評価できる。
# PathSampler.sample() の戻り値に stepwise_entropies を追加
# 例: stepwise_entropies: List[List[float]]  # 各バッチの各コモディティの各ステップ
traj_entropy = torch.tensor([e for per_batch in stepwise_entropies for e in per_batch],
                            device=y_preds.device, dtype=torch.float32)
entropy = traj_entropy.mean() if traj_entropy.numel() > 0 else torch.tensor(0.0, device=y_preds.device)
効果: “学習に効く行（実際に選んだ状態）”にだけ確実にボーナスが入る。
一気に0.000xから脱出しやすいです。

⸻

③ ε-greedy 的な「有効近傍の一様混合」を少量入れる

top-p が小さく効き過ぎると分布が尖ります。
小さな ε を足して常に少しだけ揺らすと、エントロピーが底上げされます。
# probs: [B, N_from, N_to, C]（dim=2で正規化済み）
if self.training and self.use_sampling:
    with torch.no_grad():
        # 物理的に有効な next ノード集合（invalid_mask を流用）
        valid = (~invalid_mask).unsqueeze(-1).float()
        uni = valid / (valid.sum(dim=2, keepdim=True) + 1e-8)  # 次ノード一様
    eps = 0.05  # 小さめから
    probs = (1 - eps) * probs + eps * uni
    probs = probs / (probs.sum(dim=2, keepdim=True) + 1e-8)
効果: 「候補ゼロで詰む」や「ほぼ1点集中」を回避。
PathSampler 側の top-p と相性が良いです。