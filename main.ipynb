{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "notebook_mode = True\n",
    "viz_mode = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. GPU cannot be used.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# Check if GPU is available\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA is available. GPU can be used.\")\n",
    "    print(\"Number of GPUs available:\", torch.cuda.device_count())\n",
    "    print(\"Current GPU device:\", torch.cuda.current_device())\n",
    "    print(\"GPU name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "else:\n",
    "    print(\"CUDA is not available. GPU cannot be used.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Import packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import argparse\n",
    "import time\n",
    "import random\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import torch.nn as nn\n",
    "from torch.utils.tensorboard import SummaryWriter  # tensorboardXの代替\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import networkx as nx\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "from fastprogress import master_bar, progress_bar\n",
    "\n",
    "# Remove warning\n",
    "import warnings\n",
    "#warnings.filterwarnings(\"ignore\", category=UserWarning)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "from scipy.sparse import SparseEfficiencyWarning\n",
    "warnings.simplefilter('ignore', SparseEfficiencyWarning)\n",
    "\n",
    "from config import *\n",
    "from utils.graph_utils import *\n",
    "from utils.exact_solution import SolveExactSolution\n",
    "from utils.beamsearch_uelb import *\n",
    "from utils.flow import Flow\n",
    "from utils.data_maker import DataMaker\n",
    "from utils.dataset_reader import DatasetReader\n",
    "from utils.plot_utils import *\n",
    "from models.gcn_model import ResidualGatedGCNModel\n",
    "from utils.model_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setting for notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "if notebook_mode == True:\n",
    "    %load_ext autoreload\n",
    "    %autoreload 2\n",
    "    %matplotlib inline\n",
    "    from IPython.display import set_matplotlib_formats\n",
    "    set_matplotlib_formats('png')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load configurations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded configs/default.json:\n",
      "{'expt_name': 'deafult', 'gpu_id': '0', 'graph_filepath': './data/graph.gml', 'edge_numbering_filepath': './data/edge_numbering_file.csv', 'train_filepath': './data/gragh.gml', 'val_filepath': './data/tsp10_val_concorde.txt', 'test_filepath': './data/tsp10_test_concorde.txt', 'solver_type': 'pulp', 'graph_model': 'nsfnet', 'num_train_data': 160, 'num_test_data': 20, 'num_val_data': 20, 'num_nodes': 14, 'num_neighbors': 5, 'num_commodities': 10, 'sample_size': 5, 'capacity_lower': 500, 'capacity_higher': 1000, 'demand_lower': 1, 'demand_higher': 500, 'node_dim': 10, 'voc_nodes_in': 2, 'voc_nodes_out': 2, 'voc_edges_in': 3, 'voc_edges_out': 10, 'beam_size': 10, 'hidden_dim': 50, 'num_layers': 3, 'mlp_layers': 2, 'aggregation': 'mean', 'max_epochs': 10, 'val_every': 5, 'test_every': 10, 'batch_size': 20, 'batches_per_epoch': 500, 'accumulation_steps': 1, 'learning_rate': 0.001, 'decay_rate': 1.01}\n"
     ]
    }
   ],
   "source": [
    "config_path = \"configs/default.json\"\n",
    "\n",
    "config = get_config(config_path)\n",
    "print(\"Loaded {}:\\n{}\".format(config_path, config))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configure GPU options"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA not available\n"
     ]
    }
   ],
   "source": [
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = str(config.gpu_id) \n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print(\"CUDA available, using GPU ID {}\".format(config.gpu_id))\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "    torch.cuda.manual_seed(1)\n",
    "else:\n",
    "    print(\"CUDA not available\")\n",
    "    dtypeFloat = torch.float\n",
    "    dtypeLong = torch.long\n",
    "    torch.manual_seed(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make graph dataset\n",
    "gragh.gmlとedge_numbering_file.csvの作成"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0  data was created.\n",
      "10  data was created.\n"
     ]
    }
   ],
   "source": [
    "create_data_files(config, data_mode=\"test\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(16938) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  0\n",
      "objective_value:  0.64646465\n",
      "data:  1\n",
      "objective_value:  0.66955267\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(16961) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(16962) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  2\n",
      "objective_value:  0.51051051\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(16963) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  3\n",
      "objective_value:  0.79717587\n",
      "data:  4\n",
      "objective_value:  0.7326284\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(16996) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(16997) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  5\n",
      "objective_value:  0.84347826\n",
      "data:  6\n",
      "objective_value:  0.60338346\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(16998) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(16999) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  7\n",
      "objective_value:  0.66504065\n",
      "data:  8\n",
      "objective_value:  0.76308901\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17000) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17001) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  9\n",
      "objective_value:  0.68659128\n",
      "data:  10\n",
      "objective_value:  0.76982893\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17002) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17003) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  11\n",
      "objective_value:  0.65343915\n",
      "data:  12\n",
      "objective_value:  0.72744015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17004) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17005) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  13\n",
      "objective_value:  0.65486726\n",
      "data:  14\n",
      "objective_value:  0.69658887\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17006) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17007) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  15\n",
      "objective_value:  0.42833333\n",
      "data:  16\n",
      "objective_value:  0.72591362\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17030) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17031) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  17\n",
      "objective_value:  0.62354892\n",
      "data:  18\n",
      "objective_value:  0.80614407\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "python(17032) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n",
      "python(17033) MallocStackLogging: can't turn off malloc stack logging because it was not enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data:  19\n",
      "objective_value:  0.72035139\n"
     ]
    }
   ],
   "source": [
    "for data in range(20):\n",
    "    data_mode = \"test\"\n",
    "    file_number = data - (data % 10)\n",
    "    solver_type = config.solver_type\n",
    "    graph_file_name = \"./data/{data_mode}_data/graph_file/{file_number}/graph_{data}.gml\".format(data_mode=data_mode, file_number=file_number, data=data)\n",
    "    comodity_file_name = \"./data/{data_mode}_data/commodity_file/{file_number}/commodity_data_{data}.csv\".format(data_mode=data_mode, file_number=file_number, data=data)\n",
    "    \n",
    "    # 厳密解の計算\n",
    "    E = SolveExactSolution(solver_type, comodity_file_name, graph_file_name)\n",
    "    flow_var_kakai, edge_list, objective_value, elapsed_time = E.solve_exact_solution_to_env()\n",
    "    node_flow_matrix, edge_flow_matrix, infinit_loop = E.generate_flow_matrices(flow_var_kakai)\n",
    "    print(\"data: \", data) \n",
    "    print(\"objective_value: \", objective_value)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test data loading"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of batches of size 20: 1\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'edge_file' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[104], line 12\u001b[0m\n\u001b[1;32m      9\u001b[0m t \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m     11\u001b[0m idx \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m---> 12\u001b[0m batch \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mnext\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43miter\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Generate a batch of TSPs\u001b[39;00m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mBatch generation took: \u001b[39m\u001b[38;5;132;01m{:.3f}\u001b[39;00m\u001b[38;5;124m sec\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m t))\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124medges shape:\u001b[39m\u001b[38;5;124m\"\u001b[39m, batch\u001b[38;5;241m.\u001b[39medges\u001b[38;5;241m.\u001b[39mshape)\n",
      "File \u001b[0;32m~/Desktop/Research/graph-convnet-uelb/utils/dataset_reader.py:51\u001b[0m, in \u001b[0;36mDatasetReader.__iter__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     49\u001b[0m start_idx \u001b[38;5;241m=\u001b[39m batch \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[1;32m     50\u001b[0m end_idx \u001b[38;5;241m=\u001b[39m (batch \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbatch_size\n\u001b[0;32m---> 51\u001b[0m \u001b[38;5;28;01myield\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mprocess_batch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstart_idx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mend_idx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/Desktop/Research/graph-convnet-uelb/utils/dataset_reader.py:119\u001b[0m, in \u001b[0;36mDatasetReader.process_batch\u001b[0;34m(self, start_idx, end_idx)\u001b[0m\n\u001b[1;32m    117\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\" Read edge target \"\"\"\u001b[39;00m\n\u001b[1;32m    118\u001b[0m edges \u001b[38;5;241m=\u001b[39m []\n\u001b[0;32m--> 119\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[38;5;28mopen\u001b[39m(\u001b[43medge_file\u001b[49m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mr\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01mas\u001b[39;00m edge_data:\n\u001b[1;32m    120\u001b[0m     reader \u001b[38;5;241m=\u001b[39m csv\u001b[38;5;241m.\u001b[39mreader(edge_data)\n\u001b[1;32m    121\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m row \u001b[38;5;129;01min\u001b[39;00m reader:\n",
      "\u001b[0;31mNameError\u001b[0m: name 'edge_file' is not defined"
     ]
    }
   ],
   "source": [
    "if notebook_mode:\n",
    "    mode = \"test\"\n",
    "    num_data = getattr(config, f'num_{mode}_data')\n",
    "    batch_size = config.batch_size\n",
    "    #　DataMakerを繰り返し呼び出しバッチごとにtensorを生成する\n",
    "    dataset = DatasetReader(num_data, batch_size, mode)\n",
    "    print(\"Number of batches of size {}: {}\".format(batch_size, dataset.max_iter))\n",
    "\n",
    "    t = time.time()\n",
    "    \n",
    "    idx = 0\n",
    "    batch = next(iter(dataset))  # Generate a batch of TSPs\n",
    "    print(\"Batch generation took: {:.3f} sec\".format(time.time() - t))\n",
    "    print(\"edges shape:\", batch.edges.shape)\n",
    "    print(\"edges:\", batch.edges[idx])\n",
    "    print(\"edges_capacity shape:\", batch.edges_capacity.shape)\n",
    "    print(\"edges_capacity:\", batch.edges_capacity[idx])\n",
    "    print(\"edges_targets shape:\", batch.edges_target.shape)\n",
    "    print(\"edges_targets:\", batch.edges_target[idx, :, :, idx])\n",
    "    print(\"nodes shape:\", batch.nodes.shape)\n",
    "    print(\"nodes:\", batch.nodes[idx])\n",
    "    print(\"nodes_target shape:\", batch.nodes_target.shape)\n",
    "    print(\"nodes_target:\", batch.nodes_target[idx])\n",
    "    print(\"commodities shape:\", batch.commodities.shape)\n",
    "    print(\"commodities:\", batch.commodities[idx])\n",
    "    print(\"load_factor shape:\", batch.load_factor)\n",
    "    \n",
    "    f = plt.figure(figsize=(5, 5))\n",
    "    a = f.add_subplot(111)\n",
    "    plot_uelb(a, batch.edges[idx], batch.edges_target[idx])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Instantiate model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DataParallel(\n",
      "  (module): ResidualGatedGCNModel(\n",
      "    (nodes_commodity_embedding): Linear(in_features=10, out_features=50, bias=False)\n",
      "    (edges_values_embedding): Linear(in_features=1, out_features=50, bias=False)\n",
      "    (gcn_layers): ModuleList(\n",
      "      (0-2): 3 x ResidualGatedGCNLayer(\n",
      "        (node_feat): NodeFeatures(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (edge_feat): EdgeFeatures(\n",
      "          (U): Linear(in_features=50, out_features=50, bias=True)\n",
      "          (V): Linear(in_features=50, out_features=50, bias=True)\n",
      "        )\n",
      "        (bn_node): BatchNormNode(\n",
      "          (batch_norm): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        )\n",
      "        (bn_edge): BatchNormEdge(\n",
      "          (batch_norm): BatchNorm2d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=False)\n",
      "        )\n",
      "        (relu): ReLU(inplace=True)\n",
      "      )\n",
      "    )\n",
      "    (mlp_edges): MLP(\n",
      "      (layers): ModuleList(\n",
      "        (0): Linear(in_features=50, out_features=50, bias=True)\n",
      "      )\n",
      "      (output_layer): Linear(in_features=50, out_features=10, bias=True)\n",
      "      (relu): ReLU(inplace=True)\n",
      "    )\n",
      "  )\n",
      ")\n",
      "Number of parameters: 34810\n",
      "Adam (\n",
      "Parameter Group 0\n",
      "    amsgrad: False\n",
      "    betas: (0.9, 0.999)\n",
      "    capturable: False\n",
      "    differentiable: False\n",
      "    eps: 1e-08\n",
      "    foreach: None\n",
      "    fused: None\n",
      "    lr: 0.001\n",
      "    maximize: False\n",
      "    weight_decay: 0\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if notebook_mode == True:\n",
    "    # Instantiate the network\n",
    "    net = nn.DataParallel(ResidualGatedGCNModel(config, dtypeFloat, dtypeLong))\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    print(net)\n",
    "\n",
    "    # Compute number of network parameters\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += np.prod(list(param.data.size()))\n",
    "    print('Number of parameters:', nb_param)\n",
    "    \n",
    "    # Define optimizer\n",
    "    learning_rate = config.learning_rate\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    print(optimizer)\n",
    "\n",
    "    # Enable anomaly detection\n",
    "    torch.autograd.set_detect_anomaly(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test forward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class weights: tensor([ 0.5148, 17.4067])\n",
      "Output size: torch.Size([20, 14, 14, 10])\n",
      "Loss value: tensor(6.7539, grad_fn=<MeanBackward0>)\n"
     ]
    }
   ],
   "source": [
    "if notebook_mode == True and viz_mode == False:\n",
    "    # Generate a batch of UELBs\n",
    "    mode = \"train\"\n",
    "    num_data = getattr(config, f'num_{mode}_data')\n",
    "    batch_size = config.batch_size\n",
    "    train_filepath = config.train_filepath\n",
    "    dataset = iter(DatasetReader(num_data, batch_size, mode))\n",
    "    batch = next(dataset)\n",
    "\n",
    "    # Convert batch to torch Variables\n",
    "    x_edges = torch.LongTensor(batch.edges).to(torch.long).contiguous().requires_grad_(False)\n",
    "    x_edges_capacity = torch.FloatTensor(batch.edges_capacity).to(torch.float).contiguous().requires_grad_(False)\n",
    "    x_nodes = torch.FloatTensor(batch.nodes).to(torch.float).contiguous().requires_grad_(False)\n",
    "    y_edges = torch.LongTensor(batch.edges_target).to(torch.long).contiguous().requires_grad_(False)\n",
    "    y_nodes = torch.LongTensor(batch.nodes_target).to(torch.long).contiguous().requires_grad_(False)\n",
    "    batch_commodities = torch.LongTensor(batch.commodities).to(torch.long).contiguous().requires_grad_(False)\n",
    "    \n",
    "    # Compute class weights\n",
    "    edge_labels = y_edges.cpu().numpy().flatten()\n",
    "    edge_cw = create_edge_class_weights(y_edges)\n",
    "    print(\"Class weights: {}\".format(edge_cw))\n",
    "        \n",
    "    # Enable anomaly detection\n",
    "    torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "    # Forward pass\n",
    "    y_preds, loss = net.forward(x_edges, x_edges_capacity, x_nodes, y_edges, edge_cw)\n",
    "    loss = loss.mean()\n",
    "    print(\"Output size: {}\".format(y_preds.size()))\n",
    "    #print(\"Output: {}\".format(y_preds[0, :, :, 0]))\n",
    "    print(\"Loss value:\", loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test backward pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[34], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m notebook_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m viz_mode \u001b[38;5;241m==\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# Backward pass\u001b[39;00m\n\u001b[1;32m      3\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m----> 4\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      6\u001b[0m     \u001b[38;5;66;03m# Optimizer step\u001b[39;00m\n\u001b[1;32m      7\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n",
      "File \u001b[0;32m~/miniconda3/envs/gcn-uelb-env/lib/python3.9/site-packages/torch/_tensor.py:521\u001b[0m, in \u001b[0;36mTensor.backward\u001b[0;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[1;32m    511\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[1;32m    512\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[1;32m    513\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[1;32m    514\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    519\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[1;32m    520\u001b[0m     )\n\u001b[0;32m--> 521\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    522\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[1;32m    523\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gcn-uelb-env/lib/python3.9/site-packages/torch/autograd/__init__.py:289\u001b[0m, in \u001b[0;36mbackward\u001b[0;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[1;32m    284\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[1;32m    286\u001b[0m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[1;32m    287\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[1;32m    288\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[0;32m--> 289\u001b[0m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    291\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    292\u001b[0m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    293\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    294\u001b[0m \u001b[43m    \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    295\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    296\u001b[0m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[1;32m    297\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/miniconda3/envs/gcn-uelb-env/lib/python3.9/site-packages/torch/autograd/graph.py:769\u001b[0m, in \u001b[0;36m_engine_run_backward\u001b[0;34m(t_outputs, *args, **kwargs)\u001b[0m\n\u001b[1;32m    767\u001b[0m     unregister_hooks \u001b[38;5;241m=\u001b[39m _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[1;32m    768\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 769\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[1;32m    770\u001b[0m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    771\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[1;32m    772\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m    773\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Trying to backward through the graph a second time (or directly access saved tensors after they have already been freed). Saved intermediate values of the graph are freed when you call .backward() or autograd.grad(). Specify retain_graph=True if you need to backward through the graph a second time or if you need to access saved tensors after calling backward."
     ]
    }
   ],
   "source": [
    "if notebook_mode == True and viz_mode == False:\n",
    "    # Backward pass\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    \n",
    "    # Optimizer step\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Compute error metrics \n",
    "    err_edges, err_flow, err_uelb, tour_err_idx, tsp_err_idx = edge_error(y_preds, y_edges, x_edges)\n",
    "    print(\"Edge error: {:.3f}\\nFlow error: {:.3f}\\nUELB error: {:.3f}\".format(err_edges, err_flow, err_uelb))\n",
    "    \n",
    "    # Compute mean predicted and groundtruth tour length\n",
    "    mean_maximum_load_factor = mean_load_factor(x_edges_capacity, y_preds, x_edges, batch_commodities)\n",
    "    gt_load_factor = np.mean(batch.load_factor) \n",
    "    print(\"Predicted load factor(Not always a feasible solution): {:.3f}\\nGroundtruth load factor: {:.3f}\".format(mean_maximum_load_factor, gt_load_factor))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Loop (one epoch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(net, optimizer, config, master_bar):\n",
    "    # Set training mode\n",
    "    net.train()\n",
    "\n",
    "    # Assign parameters\n",
    "    mode = \"train\"\n",
    "    num_data = getattr(config, f'num_{mode}_data')\n",
    "    num_commodities = config.num_commodities\n",
    "    num_nodes = config.num_nodes\n",
    "    beam_size = config.beam_size\n",
    "    batch_size = config.batch_size\n",
    "    batches_per_epoch = config.batches_per_epoch\n",
    "    accumulation_steps = config.accumulation_steps\n",
    "    \n",
    "    # Load UELB data\n",
    "    dataset = DatasetReader(num_data, batch_size, mode)\n",
    "    if batches_per_epoch != -1:\n",
    "        batches_per_epoch = min(batches_per_epoch, dataset.max_iter)\n",
    "    else:\n",
    "        batches_per_epoch = dataset.max_iter\n",
    "\n",
    "    # Convert dataset to iterable\n",
    "    dataset = iter(dataset)\n",
    "    \n",
    "    # Initially set loss class weights as None\n",
    "    edge_cw = None\n",
    "\n",
    "    # Initialize running data\n",
    "    running_loss = 0.0\n",
    "    # running_err_edges = 0.0\n",
    "    # running_err_tour = 0.0\n",
    "    # running_err_tsp = 0.0\n",
    "    running_mean_maximum_load_factor = 0.0\n",
    "    running_gt_load_factor = 0.0\n",
    "    running_nb_data = 0\n",
    "    running_nb_batch = 0\n",
    "\n",
    "    start_epoch = time.time()\n",
    "    for batch_num in progress_bar(range(batches_per_epoch), parent=master_bar):\n",
    "        # Generate a batch of TSPs\n",
    "        try:\n",
    "            batch = next(dataset)\n",
    "        except StopIteration:\n",
    "            break\n",
    "\n",
    "        # Convert batch to torch Variables\n",
    "        x_edges = torch.LongTensor(batch.edges).to(torch.long).contiguous().requires_grad_(False)\n",
    "        x_edges_capacity = torch.FloatTensor(batch.edges_capacity).to(torch.float).contiguous().requires_grad_(False)\n",
    "        x_nodes = torch.FloatTensor(batch.nodes).to(torch.float).contiguous().requires_grad_(False)\n",
    "        y_edges = torch.LongTensor(batch.edges_target).to(torch.long).contiguous().requires_grad_(False)\n",
    "        y_nodes = torch.LongTensor(batch.nodes_target).to(torch.long).contiguous().requires_grad_(False)\n",
    "        batch_commodities = torch.LongTensor(batch.commodities).to(torch.long).contiguous().requires_grad_(False)\n",
    "        \n",
    "        # Compute class weights (if uncomputed)\n",
    "        if type(edge_cw) != torch.Tensor:\n",
    "            edge_labels = y_edges.cpu().numpy().flatten()\n",
    "            edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "        \n",
    "        # Forward pass\n",
    "        y_preds, loss = net.forward(x_edges, x_edges_capacity, x_nodes, y_edges, edge_cw)\n",
    "        \"\"\"for beamsearch\n",
    "        beam_search = BeamsearchUELB(\n",
    "                y_preds, beam_size, batch_size, x_edges_capacity, batch_commodities, dtypeFloat, dtypeLong, strict=False) \n",
    "        bs_nodes, pred_paths = beam_search.search()\n",
    "        \"\"\"\n",
    "        loss = loss.mean()  # Take mean of loss across multiple GPUs\n",
    "        loss = loss / accumulation_steps  # Scale loss by accumulation steps\n",
    "        loss.backward()\n",
    "\n",
    "        # Backward pass\n",
    "        if (batch_num+1) % accumulation_steps == 0:\n",
    "            optimizer.step()\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "        # Compute error metrics and mean load factor\n",
    "        # err_edges, err_tour, err_tsp, tour_err_idx, tsp_err_idx = edge_error(y_preds, y_edges, x_edges)\n",
    "        #mean_maximum_load_factor = mean_feasible_load_factor(batch_size, num_commodities, num_nodes, pred_paths, x_edges_capacity, batch_commodities)\n",
    "\n",
    "        \n",
    "        gt_load_factor = np.mean(batch.load_factor) \n",
    "\n",
    "        # Update running data\n",
    "        running_nb_data += batch_size\n",
    "        running_loss += batch_size* loss.data.item()* accumulation_steps  # Re-scale loss\n",
    "        # running_err_edges += batch_size* err_edges\n",
    "        # running_err_tour += batch_size* err_tour\n",
    "        # running_err_tsp += batch_size* err_tsp\n",
    "        #running_mean_maximum_load_factor += batch_size* mean_maximum_load_factor\n",
    "        #running_gt_load_factor += batch_size* gt_load_factor\n",
    "        running_nb_batch += 1\n",
    "        \n",
    "        # Log intermediate statistics\n",
    "        \"\"\"\n",
    "        result = ('loss:{loss:.4f} mean_maximum_load_factor:{mean_maximum_load_factor:.3f} gt_load_factor:{gt_load_factor:.3f}'.format(\n",
    "            loss=running_loss/running_nb_data,\n",
    "            mean_maximum_load_factor=running_mean_maximum_load_factor/running_nb_data,\n",
    "            gt_load_factor=running_gt_load_factor/running_nb_data))\n",
    "        master_bar.child.comment = result\n",
    "        \"\"\"\n",
    "    # Compute statistics for full epoch\n",
    "    loss = running_loss/ running_nb_data\n",
    "    err_edges = 0 # running_err_edges/ running_nb_data\n",
    "    err_tour = 0 # running_err_tour/ running_nb_data\n",
    "    err_tsp = 0 # running_err_tsp/ running_nb_data\n",
    "    mean_maximum_load_factor = 0 # running_mean_maximum_load_factor/ running_nb_data\n",
    "    gt_load_factor = 0 # running_gt_load_factor/ running_nb_data\n",
    "\n",
    "    return time.time()-start_epoch, loss, err_edges, err_tour, err_tsp, mean_maximum_load_factor, gt_load_factor\n",
    "\n",
    "\n",
    "def metrics_for_train(epoch, time, learning_rate, loss):\n",
    "    result = ( 'epoch:{epoch:0>2d}\\t'\n",
    "               'time:{time:.2f}h\\t'\n",
    "               'lr:{learning_rate:.2e}\\t'\n",
    "               'loss:{loss:.4f}'.format(\n",
    "                   epoch=epoch,\n",
    "                   time=time/3600,\n",
    "                   learning_rate=learning_rate,\n",
    "                   loss=loss,\n",
    "                ))\n",
    "    return result\n",
    "    \n",
    "def metrics_to_str(epoch, time, learning_rate, loss, err_edges, err_tour, err_tsp, mean_maximum_load_factor, gt_load_factor):\n",
    "    result = ( 'epoch:{epoch:0>2d}\\t'\n",
    "               'time:{time:.1f}s\\t'\n",
    "               'lr:{learning_rate:.2e}\\t'\n",
    "               'loss:{loss:.4f}\\t'\n",
    "               # 'err_edges:{err_edges:.2f}\\t'\n",
    "               # 'err_tour:{err_tour:.2f}\\t'\n",
    "               # 'err_tsp:{err_tsp:.2f}\\t'\n",
    "               'mean_maximum_load_factor:{mean_maximum_load_factor:.3f}\\t'\n",
    "               'gt_load_factor:{gt_load_factor:.3f}'.format(\n",
    "                   epoch=epoch,\n",
    "                   time=time/3600,\n",
    "                   learning_rate=learning_rate,\n",
    "                   loss=loss,\n",
    "                   # err_edges=err_edges,\n",
    "                   # err_tour=err_tour,\n",
    "                   # err_tsp=err_tsp,\n",
    "                   mean_maximum_load_factor=mean_maximum_load_factor,\n",
    "                   gt_load_factor=gt_load_factor))\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    /* Turns off some styling */\n",
       "    progress {\n",
       "        /* gets rid of default border in Firefox and Opera. */\n",
       "        border: none;\n",
       "        /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
       "        background-size: auto;\n",
       "    }\n",
       "    progress:not([value]), progress:not([value])::-webkit-progress-bar {\n",
       "        background: repeating-linear-gradient(45deg, #7e7e7e, #7e7e7e 10px, #5c5c5c 10px, #5c5c5c 20px);\n",
       "    }\n",
       "    .progress-bar-interrupted, .progress-bar-interrupted::-webkit-progress-bar {\n",
       "        background: #F44336;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='1' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/1 00:00&lt;?]\n",
       "    </div>\n",
       "    \n",
       "\n",
       "\n",
       "    <div>\n",
       "      <progress value='0' class='' max='8' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      0.00% [0/8 00:00&lt;?]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "max_iter:  8\n"
     ]
    }
   ],
   "source": [
    "if notebook_mode==True and viz_mode==False:\n",
    "    epoch_bar = master_bar(range(1))\n",
    "    for epoch in epoch_bar:\n",
    "        train_time, train_loss, train_err_edges, train_err_tour, train_err_tsp, mean_maximum_load_factor, gt_load_factor = train_one_epoch(net, optimizer, config, epoch_bar)\n",
    "        epoch_bar.write('t: ' + metrics_for_train(epoch, train_time, learning_rate, train_loss))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Loop (for validation and test sets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bs_edges_summed:  tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,  31, 348,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [185, 348,   0,   0,   0,  31,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0, 425,   0,   0, 348,   0,   0,   0,   0,   0, 156,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0, 452, 404,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0, 533,   0, 404,   0,   0,   0,   0, 483,   0,   0, 357,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0, 201,   0,   0,   0,   0,   0, 357],\n",
      "        [  0,   0,   0,   0,   0, 533,   0,   0, 452,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0, 424,   0,   0,   0,   0,   0,   0,   0,   0,   0, 156],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 357,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0, 404,   0,   0,   0,   0,   0, 357,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 156,   0, 424,   0,   0,   0]])\n",
      "max_values_per_batch:  tensor([0.6522])\n",
      "Mean Feasible Load Factor: tensor(0.6522)\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# サンプルデータ\n",
    "bs_node = torch.tensor([[\n",
    "    [ 0,  0,  0,  0,  1,  0,  2,  3,  4,  0,  0,  0,  0,  0],\n",
    "    [ 4,  5,  0,  0,  0,  0,  0,  3,  2,  0,  0,  0,  0,  1],\n",
    "    [ 0,  0,  0,  4,  5,  0,  6,  0,  0,  0,  3,  2,  1,  0],\n",
    "    [ 0,  0,  0,  0,  0,  1,  0,  0,  0,  0,  0,  0,  2,  3],\n",
    "    [ 0,  0,  0,  0,  3,  2,  0,  0,  0,  1,  0,  0,  0,  0],\n",
    "    [ 8,  6,  7,  5,  4,  3,  0,  9,  1,  2,  0,  0,  0,  0],\n",
    "    [ 4,  0,  3,  0,  0,  2,  0,  0,  0,  1,  0,  0,  0,  0],\n",
    "    [ 0,  0,  0,  1,  0,  0,  0,  0,  4,  0,  2,  3,  0,  0],\n",
    "    [ 0,  1,  2,  0,  0,  3,  0,  0,  0,  4,  0,  0,  0,  0],\n",
    "    [ 0,  2,  0,  1,  0,  0,  0,  0,  0,  0,  0,  0,  0,  0]\n",
    "]])\n",
    "# サンプルデータ\n",
    "#pred_paths = [[[4, 6, 7, 8], [13, 8, 9, 5, 4, 6, 7, 0, 2, 1], [12, 13, 8, 7, 6], [5, 4, 6, 7, 0, 2, 1, 3, 10, 11, 8, 13], [9, 5, 4], [8, 13, 10, 3, 1, 2, 5, 4, 6, 7], [9, 8, 13, 10, 3, 1, 2, 5, 4, 6, 7, 0], [3, 10, 11, 8], [1, 3, 4, 6, 7, 0, 2, 5, 12, 11, 10, 13, 8, 9], [3, 1]]]\n",
    "pred_paths = [[[4, 5, 9, 8], [13, 10, 3, 1], [12, 5, 4, 6], [5, 12, 11, 8, 13], [9, 5, 2, 1, 3, 4], [8, 7], [9, 5, 2, 0], [3, 10, 13, 8], [1, 2, 5, 9], [3, 1]]]\n",
    "# 厳密解のフロー\n",
    "bs_node = torch.tensor([[\n",
    "    [0, 0, 0, 0, 1, 2, 0, 0, 4, 3, 0, 0, 0, 0],\n",
    "    [0, 4, 0, 3, 0, 0, 0, 0, 0, 0, 2, 0, 0, 1],\n",
    "    [0, 0, 0, 0, 3, 2, 4, 0, 0, 0, 0, 0, 1, 0],\n",
    "    [0, 0, 0, 0, 0, 1, 0, 0, 4, 0, 0, 3, 2, 5],\n",
    "    [0, 4, 3, 5, 6, 2, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 0, 0, 0, 0, 2, 1, 0, 0, 0, 0, 0],\n",
    "    [4, 0, 3, 0, 0, 2, 0, 0, 0, 1, 0, 0, 0, 0],\n",
    "    [0, 0, 0, 1, 0, 0, 0, 0, 4, 0, 2, 0, 0, 3],\n",
    "    [0, 1, 2, 0, 0, 3, 0, 0, 0, 4, 0, 0, 0, 0],\n",
    "    [0, 2, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]\n",
    "]])\n",
    "\n",
    "commodities = torch.tensor([[\n",
    "    [4, 8, 452],\n",
    "    [13, 1, 424],\n",
    "    [12, 6, 404],\n",
    "    [5, 13, 357],\n",
    "    [9, 4, 348],\n",
    "    [8, 7, 201],\n",
    "    [9, 0, 185],\n",
    "    [3, 8, 156],\n",
    "    [1, 9, 31],\n",
    "    [3, 1, 1]\n",
    "    ]])\n",
    "num_batch = 1\n",
    "num_flow = 10\n",
    "num_node = 14\n",
    "\n",
    "edges_capacity = torch.tensor([[\n",
    "    [0., 737., 713., 0., 0., 0., 0., 789., 0., 0., 0., 0., 0., 0.],\n",
    "    [737., 0., 801., 915., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [713., 801., 0., 0., 0., 924., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 915., 0., 0., 560., 0., 0., 0., 0., 0., 766., 0., 0., 0.],\n",
    "    [0., 0., 0., 560., 0., 993., 963., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 924., 0., 993., 0., 0., 0., 0., 840., 0., 0., 742., 0.],\n",
    "    [0., 0., 0., 0., 963., 0., 0., 801., 0., 0., 0., 0., 0., 0.],\n",
    "    [789., 0., 0., 0., 0., 0., 801., 0., 575., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 575., 0., 693., 0., 630., 0., 604.],\n",
    "    [0., 0., 0., 0., 0., 840., 0., 0., 693., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 766., 0., 0., 0., 0., 0., 0., 0., 511., 0., 913.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 630., 0., 511., 0., 629., 0.],\n",
    "    [0., 0., 0., 0., 0., 742., 0., 0., 0., 0., 0., 629., 0., 500.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 604., 0., 913., 0., 500., 0.]\n",
    "]])\n",
    "# テスト用に関数を呼び出し\n",
    "mean_max_load_factor = mean_feasible_load_factor(num_batch, num_flow, num_node, pred_paths, edges_capacity, commodities)\n",
    "print(\"Mean Feasible Load Factor:\", mean_max_load_factor)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ビームサーチ＆エッジの教師データの有効性検証"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "def beam_test(net, config, master_bar, mode='train'):\n",
    "    # Set evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Assign parameters\n",
    "    num_data = getattr(config, f'num_{mode}_data')\n",
    "    #batch_size = config.batch_size\n",
    "    #num_data = 1\n",
    "    batch_size = 1\n",
    "    num_commodities = config.num_commodities\n",
    "    num_nodes = config.num_nodes\n",
    "    beam_size = config.beam_size\n",
    "    #batches_per_epoch = config.batches_per_epoch\n",
    "    batches_per_epoch = 1\n",
    "    accumulation_steps = config.accumulation_steps\n",
    "    \n",
    "    # Load UELB data\n",
    "    dataset = DatasetReader(num_data, batch_size, mode)\n",
    "    \n",
    "        # Convert dataset to iterable\n",
    "    dataset = iter(dataset)\n",
    "    \n",
    "    # Initially set loss class weights as None\n",
    "    edge_cw = None\n",
    "\n",
    "    # Initialize running data\n",
    "    running_loss = 0.0\n",
    "    running_mean_maximum_load_factor = 0.0\n",
    "    running_gt_load_factor = 0.0\n",
    "    running_nb_data = 0\n",
    "    running_nb_batch = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_test = time.time()\n",
    "        for batch_num in progress_bar(range(batches_per_epoch), parent=master_bar):\n",
    "            print(\"batch_num: \", batch_num)\n",
    "            # Generate a batch of TSPs\n",
    "            try:\n",
    "                batch = next(dataset)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            # Convert batch to torch Variables\n",
    "            x_edges_capacity = torch.FloatTensor(batch.edges_capacity).to(torch.float).contiguous().requires_grad_(False)\n",
    "            y_edges = torch.LongTensor(batch.edges_target).to(torch.long).contiguous().requires_grad_(False)\n",
    "            batch_commodities = torch.LongTensor(batch.commodities).to(torch.long).contiguous().requires_grad_(False)     \n",
    "\n",
    "            # Compute class weights (if uncomputed)\n",
    "            if type(edge_cw) != torch.Tensor:\n",
    "                edge_labels = y_edges.cpu().numpy().flatten()\n",
    "                edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "                \n",
    "\n",
    "            beam_search = BeamsearchUELB(\n",
    "                y_edges, beam_size, batch_size, x_edges_capacity, batch_commodities, dtypeFloat, dtypeLong, mode_strict=True) \n",
    "            #bs_nodes, pred_paths = beam_search.search()\n",
    "            pred_paths = beam_search.search()\n",
    "            torch.set_printoptions(linewidth=200)\n",
    "            for i in range(10):\n",
    "                print(f\"y_edges[0, :, :, {i}]:\\n\", y_edges[0, :, :, 0])\n",
    "            print(\"pred_paths:\\n\", pred_paths)\n",
    "            \n",
    "            #if batch_num == 0:\n",
    "                #print(\"bs_nodes.shape: \", bs_nodes.shape)\n",
    "                #print(\"bs_nodes:\\n\", bs_nodes)\n",
    "                #print(\"pred_paths:\\n\", pred_paths)\n",
    "            \n",
    "            # Compute error metrics and mean load factor\n",
    "            # err_edges, err_tour, err_tsp, tour_err_idx, tsp_err_idx = edge_error(y_preds, y_edges, x_edges)\n",
    "            mean_maximum_load_factor = mean_feasible_load_factor(batch_size, num_commodities, num_nodes, pred_paths, x_edges_capacity, batch_commodities)\n",
    "            print(\"mean_maximum_load_factor: \", mean_maximum_load_factor)\n",
    "            gt_load_factor = np.mean(batch.load_factor)\n",
    "            print(\"gt_load_factor: \", gt_load_factor)\n",
    "            #if mean_maximum_load_factor < gt_load_factor:\n",
    "               #print(\"x_edges_capacity:\\n\", x_edges_capacity)\n",
    "                #print(\"batch_commodities:\\n\", batch_commodities)\n",
    "                #print(\"pred_paths:\\n\", pred_paths)\n",
    "        \n",
    "            \n",
    "    # Compute statistics for full epoch\n",
    "\n",
    "\n",
    "    return mean_maximum_load_factor, gt_load_factor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch_num:  0\n",
      "y_edges[0, :, :, 0]:\n",
      " tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "y_edges[0, :, :, 1]:\n",
      " tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "y_edges[0, :, :, 2]:\n",
      " tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "y_edges[0, :, :, 3]:\n",
      " tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "y_edges[0, :, :, 4]:\n",
      " tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "y_edges[0, :, :, 5]:\n",
      " tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "y_edges[0, :, :, 6]:\n",
      " tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "y_edges[0, :, :, 7]:\n",
      " tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "y_edges[0, :, :, 8]:\n",
      " tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "y_edges[0, :, :, 9]:\n",
      " tensor([[0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0],\n",
      "        [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0]])\n",
      "pred_paths:\n",
      " [[[0, 1, 2, 5, 4, 3], [9, 8, 7], [11, 12, 5, 2, 0], [4, 5], [0, 7, 8, 13], [7, 8, 13, 10], [11, 10, 3, 4], [2, 5, 12, 11, 8], [6, 4], [12, 13]]]\n",
      "mean_maximum_load_factor:  tensor(0.6465)\n",
      "gt_load_factor:  0.61525705\n"
     ]
    }
   ],
   "source": [
    "val_pred_tour_len, val_gt_tour_len = beam_test(net, config, epoch_bar, mode='test')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test(net, config, master_bar, mode='test'):\n",
    "    # Set evaluation mode\n",
    "    net.eval()\n",
    "    \n",
    "    # Assign parameters\n",
    "    num_data = getattr(config, f'num_{mode}_data')\n",
    "    batch_size = config.batch_size\n",
    "    #num_data = 1\n",
    "    #batch_size = 1\n",
    "    num_commodities = config.num_commodities\n",
    "    num_nodes = config.num_nodes\n",
    "    beam_size = config.beam_size\n",
    "    batches_per_epoch = config.batches_per_epoch\n",
    "    accumulation_steps = config.accumulation_steps\n",
    "    \n",
    "    # Load UELB data\n",
    "    dataset = DatasetReader(num_data, batch_size, mode)\n",
    "    \n",
    "        # Convert dataset to iterable\n",
    "    dataset = iter(dataset)\n",
    "    \n",
    "    # Initially set loss class weights as None\n",
    "    edge_cw = None\n",
    "\n",
    "    # Initialize running data\n",
    "    running_loss = 0.0\n",
    "    running_mean_maximum_load_factor = 0.0\n",
    "    running_gt_load_factor = 0.0\n",
    "    running_nb_data = 0\n",
    "    running_nb_batch = 0\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        start_test = time.time()\n",
    "        for batch_num in progress_bar(range(batches_per_epoch), parent=master_bar):\n",
    "            # Generate a batch of TSPs\n",
    "            try:\n",
    "                batch = next(dataset)\n",
    "            except StopIteration:\n",
    "                break\n",
    "\n",
    "            # Convert batch to torch Variables\n",
    "            x_edges = torch.LongTensor(batch.edges).to(torch.long).contiguous().requires_grad_(False)\n",
    "            x_edges_capacity = torch.FloatTensor(batch.edges_capacity).to(torch.float).contiguous().requires_grad_(False)\n",
    "            x_nodes = torch.FloatTensor(batch.nodes).to(torch.float).contiguous().requires_grad_(False)\n",
    "            y_edges = torch.LongTensor(batch.edges_target).to(torch.long).contiguous().requires_grad_(False)\n",
    "            y_nodes = torch.LongTensor(batch.nodes_target).to(torch.long).contiguous().requires_grad_(False)\n",
    "            batch_commodities = torch.LongTensor(batch.commodities).to(torch.long).contiguous().requires_grad_(False)     \n",
    "\n",
    "            # Compute class weights (if uncomputed)\n",
    "            if type(edge_cw) != torch.Tensor:\n",
    "                edge_labels = y_edges.cpu().numpy().flatten()\n",
    "                edge_cw = compute_class_weight(\"balanced\", classes=np.unique(edge_labels), y=edge_labels)\n",
    "                \n",
    "\n",
    "            # Forward pass\n",
    "            y_preds, loss = net.forward(x_edges, x_edges_capacity, x_nodes, y_edges, edge_cw)\n",
    "            loss = loss.mean()  # Take mean of loss across multiple GPUs    \n",
    "            beam_search = BeamsearchUELB(\n",
    "                y_preds, beam_size, batch_size, x_edges_capacity, batch_commodities, dtypeFloat, dtypeLong, mode_strict=True) \n",
    "            #bs_nodes, pred_paths = beam_search.search()\n",
    "            pred_paths = beam_search.search()\n",
    "            \n",
    "            #if batch_num == 0:\n",
    "                #print(\"bs_nodes.shape: \", bs_nodes.shape)\n",
    "                #print(\"bs_nodes:\\n\", bs_nodes)\n",
    "                #print(\"pred_paths:\\n\", pred_paths)\n",
    "            \n",
    "            # Compute error metrics and mean load factor\n",
    "            # err_edges, err_tour, err_tsp, tour_err_idx, tsp_err_idx = edge_error(y_preds, y_edges, x_edges)\n",
    "            mean_maximum_load_factor = mean_feasible_load_factor(batch_size, num_commodities, num_nodes, pred_paths, x_edges_capacity, batch_commodities)\n",
    "            print(\"mean_maximum_load_factor: \", mean_maximum_load_factor)\n",
    "            gt_load_factor = np.mean(batch.load_factor) \n",
    "\n",
    "            # Update running data\n",
    "            running_nb_data += batch_size\n",
    "            running_loss += batch_size* loss.data.item()* accumulation_steps  # Re-scale loss\n",
    "            running_mean_maximum_load_factor += batch_size* mean_maximum_load_factor\n",
    "            running_gt_load_factor += batch_size* gt_load_factor\n",
    "            running_nb_batch += 1\n",
    "        \n",
    "            # Log intermediate statistics\n",
    "            result = ('loss:{loss:.4f} mean_maximum_load_factor:{mean_maximum_load_factor:.3f} gt_load_factor:{gt_load_factor:.3f}'.format(\n",
    "                loss=running_loss/running_nb_data,\n",
    "                mean_maximum_load_factor=running_mean_maximum_load_factor/running_nb_data,\n",
    "                gt_load_factor=running_gt_load_factor/running_nb_data))\n",
    "            master_bar.child.comment = result\n",
    "            \n",
    "    # Compute statistics for full epoch\n",
    "    loss = running_loss/ running_nb_data\n",
    "    err_edges = 0 # running_err_edges/ running_nb_data\n",
    "    err_tour = 0 # running_err_tour/ running_nb_data\n",
    "    err_tsp = 0 # running_err_tsp/ running_nb_data\n",
    "    mean_maximum_load_factor = running_mean_maximum_load_factor/ running_nb_data\n",
    "    gt_load_factor = running_gt_load_factor/ running_nb_data\n",
    "\n",
    "    return time.time()-start_test, loss, err_edges, err_tour, err_tsp, mean_maximum_load_factor, gt_load_factor\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "v: epoch:00\ttime:0.0h\tlr:1.00e-03\tloss:0.0973\tmean_maximum_load_factor:0.816\tgt_load_factor:0.628"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "demand:  404\n",
      "best_path:  [12, 13, 8, 7, 6]\n",
      "remaining_edges_capacity:  tensor([[  0., 737., 713.,   0.,   0.,   0.,   0., 789.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [737.,   0., 801., 915.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [713., 801.,   0.,   0.,   0., 924.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 915.,   0.,   0., 560.,   0.,   0.,   0.,   0.,   0., 766.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 560.,   0., 993., 963.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 924.,   0., 993.,   0.,   0.,   0.,   0., 840.,   0.,   0., 742.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 963.,   0.,   0., 801.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [789.,   0.,   0.,   0.,   0.,   0., 397.,   0., 575.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171.,   0., 693.,   0., 630.,   0., 604.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 840.,   0.,   0., 693.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 766.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 511.,   0., 913.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 630.,   0., 511.,   0., 629.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 742.,   0.,   0.,   0.,   0.,   0., 629.,   0.,  96.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200.,   0., 913.,   0., 500.,   0.]])\n",
      "demand:  201\n",
      "best_path:  [8, 11, 10, 3, 4, 6, 7]\n",
      "remaining_edges_capacity:  tensor([[  0., 737., 713.,   0.,   0.,   0.,   0., 789.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [737.,   0., 801., 915.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [713., 801.,   0.,   0.,   0., 924.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 915.,   0.,   0., 359.,   0.,   0.,   0.,   0.,   0., 766.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 560.,   0., 993., 762.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 924.,   0., 993.,   0.,   0.,   0.,   0., 840.,   0.,   0., 742.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 963.,   0.,   0., 600.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [789.,   0.,   0.,   0.,   0.,   0., 397.,   0., 575.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171.,   0., 693.,   0., 429.,   0., 604.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 840.,   0.,   0., 693.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 565.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 511.,   0., 913.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 630.,   0., 310.,   0., 629.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 742.,   0.,   0.,   0.,   0.,   0., 629.,   0.,  96.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200.,   0., 913.,   0., 500.,   0.]])\n",
      "demand:  357\n",
      "best_path:  [5, 9, 8, 13]\n",
      "remaining_edges_capacity:  tensor([[  0., 737., 713.,   0.,   0.,   0.,   0., 789.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [737.,   0., 801., 915.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [713., 801.,   0.,   0.,   0., 924.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 915.,   0.,   0., 359.,   0.,   0.,   0.,   0.,   0., 766.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 560.,   0., 993., 762.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 924.,   0., 993.,   0.,   0.,   0.,   0., 483.,   0.,   0., 742.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 963.,   0.,   0., 600.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [789.,   0.,   0.,   0.,   0.,   0., 397.,   0., 575.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171.,   0., 693.,   0., 429.,   0., 247.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 840.,   0.,   0., 336.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 565.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 511.,   0., 913.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 630.,   0., 310.,   0., 629.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 742.,   0.,   0.,   0.,   0.,   0., 629.,   0.,  96.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200.,   0., 913.,   0., 500.,   0.]])\n",
      "demand:  348\n",
      "best_path:  [9, 5, 4]\n",
      "remaining_edges_capacity:  tensor([[  0., 737., 713.,   0.,   0.,   0.,   0., 789.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [737.,   0., 801., 915.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [713., 801.,   0.,   0.,   0., 924.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 915.,   0.,   0., 359.,   0.,   0.,   0.,   0.,   0., 766.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 560.,   0., 993., 762.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 924.,   0., 645.,   0.,   0.,   0.,   0., 483.,   0.,   0., 742.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 963.,   0.,   0., 600.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [789.,   0.,   0.,   0.,   0.,   0., 397.,   0., 575.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171.,   0., 693.,   0., 429.,   0., 247.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 492.,   0.,   0., 336.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 565.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 511.,   0., 913.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 630.,   0., 310.,   0., 629.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 742.,   0.,   0.,   0.,   0.,   0., 629.,   0.,  96.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200.,   0., 913.,   0., 500.,   0.]])\n",
      "demand:  452\n",
      "best_path:  [4, 6, 7, 8]\n",
      "remaining_edges_capacity:  tensor([[  0., 737., 713.,   0.,   0.,   0.,   0., 789.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [737.,   0., 801., 915.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [713., 801.,   0.,   0.,   0., 924.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 915.,   0.,   0., 359.,   0.,   0.,   0.,   0.,   0., 766.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 560.,   0., 993., 310.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 924.,   0., 645.,   0.,   0.,   0.,   0., 483.,   0.,   0., 742.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 963.,   0.,   0., 148.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [789.,   0.,   0.,   0.,   0.,   0., 397.,   0., 123.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171.,   0., 693.,   0., 429.,   0., 247.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 492.,   0.,   0., 336.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 565.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 511.,   0., 913.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 630.,   0., 310.,   0., 629.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 742.,   0.,   0.,   0.,   0.,   0., 629.,   0.,  96.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200.,   0., 913.,   0., 500.,   0.]])\n",
      "demand:  424\n",
      "best_path:  [13, 10, 3, 1]\n",
      "remaining_edges_capacity:  tensor([[  0., 737., 713.,   0.,   0.,   0.,   0., 789.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [737.,   0., 801., 915.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [713., 801.,   0.,   0.,   0., 924.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 491.,   0.,   0., 359.,   0.,   0.,   0.,   0.,   0., 766.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 560.,   0., 993., 310.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 924.,   0., 645.,   0.,   0.,   0.,   0., 483.,   0.,   0., 742.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 963.,   0.,   0., 148.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [789.,   0.,   0.,   0.,   0.,   0., 397.,   0., 123.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171.,   0., 693.,   0., 429.,   0., 247.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 492.,   0.,   0., 336.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 511.,   0., 913.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 630.,   0., 310.,   0., 629.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 742.,   0.,   0.,   0.,   0.,   0., 629.,   0.,  96.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200.,   0., 489.,   0., 500.,   0.]])\n",
      "demand:  1\n",
      "best_path:  [3, 1]\n",
      "remaining_edges_capacity:  tensor([[  0., 737., 713.,   0.,   0.,   0.,   0., 789.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [737.,   0., 801., 915.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [713., 801.,   0.,   0.,   0., 924.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 490.,   0.,   0., 359.,   0.,   0.,   0.,   0.,   0., 766.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 560.,   0., 993., 310.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 924.,   0., 645.,   0.,   0.,   0.,   0., 483.,   0.,   0., 742.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 963.,   0.,   0., 148.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [789.,   0.,   0.,   0.,   0.,   0., 397.,   0., 123.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171.,   0., 693.,   0., 429.,   0., 247.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 492.,   0.,   0., 336.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 511.,   0., 913.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 630.,   0., 310.,   0., 629.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 742.,   0.,   0.,   0.,   0.,   0., 629.,   0.,  96.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200.,   0., 489.,   0., 500.,   0.]])\n",
      "demand:  185\n",
      "best_path:  [9, 5, 2, 0]\n",
      "remaining_edges_capacity:  tensor([[  0., 737., 713.,   0.,   0.,   0.,   0., 789.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [737.,   0., 801., 915.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [528., 801.,   0.,   0.,   0., 924.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 490.,   0.,   0., 359.,   0.,   0.,   0.,   0.,   0., 766.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 560.,   0., 993., 310.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 739.,   0., 645.,   0.,   0.,   0.,   0., 483.,   0.,   0., 742.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 963.,   0.,   0., 148.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [789.,   0.,   0.,   0.,   0.,   0., 397.,   0., 123.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171.,   0., 693.,   0., 429.,   0., 247.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 307.,   0.,   0., 336.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 511.,   0., 913.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 630.,   0., 310.,   0., 629.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 742.,   0.,   0.,   0.,   0.,   0., 629.,   0.,  96.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200.,   0., 489.,   0., 500.,   0.]])\n",
      "demand:  156\n",
      "best_path:  [3, 10, 11, 8]\n",
      "remaining_edges_capacity:  tensor([[  0., 737., 713.,   0.,   0.,   0.,   0., 789.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [737.,   0., 801., 915.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [528., 801.,   0.,   0.,   0., 924.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 490.,   0.,   0., 359.,   0.,   0.,   0.,   0.,   0., 610.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 560.,   0., 993., 310.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 739.,   0., 645.,   0.,   0.,   0.,   0., 483.,   0.,   0., 742.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 963.,   0.,   0., 148.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [789.,   0.,   0.,   0.,   0.,   0., 397.,   0., 123.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171.,   0., 693.,   0., 429.,   0., 247.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 307.,   0.,   0., 336.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 355.,   0., 913.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 474.,   0., 310.,   0., 629.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 742.,   0.,   0.,   0.,   0.,   0., 629.,   0.,  96.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200.,   0., 489.,   0., 500.,   0.]])\n",
      "demand:  31\n",
      "best_path:  [1, 2, 5, 9]\n",
      "remaining_edges_capacity:  tensor([[  0., 737., 713.,   0.,   0.,   0.,   0., 789.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [737.,   0., 770., 915.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [528., 801.,   0.,   0.,   0., 893.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 490.,   0.,   0., 359.,   0.,   0.,   0.,   0.,   0., 610.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 560.,   0., 993., 310.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 739.,   0., 645.,   0.,   0.,   0.,   0., 452.,   0.,   0., 742.,   0.],\n",
      "        [  0.,   0.,   0.,   0., 963.,   0.,   0., 148.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [789.,   0.,   0.,   0.,   0.,   0., 397.,   0., 123.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 171.,   0., 693.,   0., 429.,   0., 247.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 307.,   0.,   0., 336.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 141.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 355.,   0., 913.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 474.,   0., 310.,   0., 629.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 742.,   0.,   0.,   0.,   0.,   0., 629.,   0.,  96.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 200.,   0., 489.,   0., 500.,   0.]])\n",
      "各エッジで流れた容量:  tensor([[  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,  31.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [185.,   0.,   0.,   0.,   0.,  31.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0., 425.,   0.,   0., 201.,   0.,   0.,   0.,   0.,   0., 156.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0., 653.,   0.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0., 185.,   0., 348.,   0.,   0.,   0.,   0., 388.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 653.,   0.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0., 404.,   0., 452.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0., 404.,   0.,   0.,   0., 201.,   0., 357.],\n",
      "        [  0.,   0.,   0.,   0.,   0., 533.,   0.,   0., 357.,   0.,   0.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0., 625.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 156.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 156.,   0., 201.,   0.,   0.,   0.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 404.],\n",
      "        [  0.,   0.,   0.,   0.,   0.,   0.,   0.,   0., 404.,   0., 424.,   0.,   0.,   0.]])\n",
      "pred_paths:  [[[4, 6, 7, 8], [13, 10, 3, 1], [12, 13, 8, 7, 6], [5, 9, 8, 13], [9, 5, 4], [8, 11, 10, 3, 4, 6, 7], [9, 5, 2, 0], [3, 10, 11, 8], [1, 2, 5, 9], [3, 1]]]\n",
      "bs_edges_summed:  tensor([[  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,  31,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [185,   0,   0,   0,   0,  31,   0,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0, 425,   0,   0, 201,   0,   0,   0,   0,   0, 156,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0, 653,   0,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0, 185,   0, 348,   0,   0,   0,   0, 388,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0, 653,   0,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0, 404,   0, 452,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0, 404,   0,   0,   0, 201,   0, 357],\n",
      "        [  0,   0,   0,   0,   0, 533,   0,   0, 357,   0,   0,   0,   0,   0],\n",
      "        [  0,   0,   0, 625,   0,   0,   0,   0,   0,   0,   0, 156,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 156,   0, 201,   0,   0,   0],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0,   0, 404],\n",
      "        [  0,   0,   0,   0,   0,   0,   0,   0, 404,   0, 424,   0,   0,   0]])\n",
      "mean_maximum_load_factor:  tensor(0.8159)\n"
     ]
    }
   ],
   "source": [
    "if notebook_mode==True and viz_mode==False:\n",
    "    epoch_bar = master_bar(range(1))\n",
    "    for epoch in epoch_bar:\n",
    "        # Validation\n",
    "        val_time, val_loss, val_err_edges, val_err_tour, val_err_tsp, val_pred_tour_len, val_gt_tour_len = test(net, config, epoch_bar, mode='val')\n",
    "        epoch_bar.write('v: ' + metrics_to_str(epoch, val_time, learning_rate, val_loss, val_err_edges, val_err_tour, val_err_tsp, val_pred_tour_len, val_gt_tour_len))\n",
    "        # Testing\n",
    "        test_time, test_loss, test_err_edges, test_err_tour, test_err_tsp, test_pred_tour_len, test_gt_tour_len = test(net, config, epoch_bar, mode='test')\n",
    "        epoch_bar.write('T: ' + metrics_to_str(epoch, test_time, learning_rate, test_loss, test_err_edges, test_err_tour, test_err_tsp, test_pred_tour_len, test_gt_tour_len))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "edges_capacity = torch.tensor([[\n",
    "    [0., 737., 713., 0., 0., 0., 0., 789., 0., 0., 0., 0., 0., 0.],\n",
    "    [737., 0., 801., 915., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [713., 801., 0., 0., 0., 924., 0., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 915., 0., 0., 560., 0., 0., 0., 0., 0., 766., 0., 0., 0.],\n",
    "    [0., 0., 0., 560., 0., 993., 963., 0., 0., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 924., 0., 993., 0., 0., 0., 0., 840., 0., 0., 742., 0.],\n",
    "    [0., 0., 0., 0., 963., 0., 0., 801., 0., 0., 0., 0., 0., 0.],\n",
    "    [789., 0., 0., 0., 0., 0., 801., 0., 575., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 575., 0., 693., 0., 630., 0., 604.],\n",
    "    [0., 0., 0., 0., 0., 840., 0., 0., 693., 0., 0., 0., 0., 0.],\n",
    "    [0., 0., 0., 766., 0., 0., 0., 0., 0., 0., 0., 511., 0., 913.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 630., 0., 511., 0., 629., 0.],\n",
    "    [0., 0., 0., 0., 0., 742., 0., 0., 0., 0., 0., 629., 0., 500.],\n",
    "    [0., 0., 0., 0., 0., 0., 0., 0., 604., 0., 913., 0., 500., 0.]\n",
    "]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Run full training pipeline "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "def main(config):\n",
    "    # Instantiate the network\n",
    "    net = nn.DataParallel(ResidualGatedGCNModel(config, dtypeFloat, dtypeLong))\n",
    "    if torch.cuda.is_available():\n",
    "        net.cuda()\n",
    "    print(net)\n",
    "\n",
    "    # Compute number of network parameters\n",
    "    nb_param = 0\n",
    "    for param in net.parameters():\n",
    "        nb_param += np.prod(list(param.data.size()))\n",
    "    print('Number of parameters:', nb_param)\n",
    " \n",
    "    # Create log directory\n",
    "    log_dir = f\"./logs/{config.expt_name}/\"\n",
    "    os.makedirs(log_dir, exist_ok=True)\n",
    "    json.dump(config, open(f\"{log_dir}/config.json\", \"w\"), indent=4)\n",
    "    writer = SummaryWriter(log_dir)  # Define Tensorboard writer\n",
    "    \n",
    "    # Training parameters\n",
    "    #num_nodes = config.num_nodes\n",
    "    #num_neighbors = config.num_neighbors\n",
    "    max_epochs = config.max_epochs\n",
    "    val_every = config.val_every\n",
    "    test_every = config.test_every\n",
    "    batch_size = config.batch_size\n",
    "    batches_per_epoch = config.batches_per_epoch\n",
    "    accumulation_steps = config.accumulation_steps\n",
    "    learning_rate = config.learning_rate\n",
    "    decay_rate = config.decay_rate\n",
    "    val_loss_old = 1e6  # For decaying LR based on validation loss\n",
    "    best_pred_tour_len = 1e6  # For saving checkpoints\n",
    "    \n",
    "    # Define optimizer\n",
    "    optimizer = torch.optim.Adam(net.parameters(), lr=learning_rate)\n",
    "    print(optimizer)\n",
    "    \n",
    "    epoch_bar = master_bar(range(max_epochs))\n",
    "    loss_transition = []\n",
    "    load_factor_transition = []\n",
    "    for epoch in epoch_bar:\n",
    "        # Log to Tensorboard\n",
    "        writer.add_scalar('learning_rate', learning_rate, epoch)\n",
    "        \n",
    "        # Train\n",
    "        train_time, train_loss, train_err_edges, train_err_tour, train_err_tsp, train_mean_maximum_load_factor, train_gt_load_factor = train_one_epoch(net, optimizer, config, epoch_bar)\n",
    "        #epoch_bar.write('train: ' + metrics_to_str(epoch, train_time, learning_rate, train_loss, train_err_edges, train_err_tour, train_err_tsp, train_mean_maximum_load_factor, train_gt_load_factor))\n",
    "        epoch_bar.write('train: ' + metrics_for_train(epoch, train_time, learning_rate, train_loss))\n",
    "        #writer.add_scalar('loss/train_loss', train_loss, epoch)\n",
    "        #writer.add_scalar('pred_tour_len/train_mean_maximum_load_factor', train_mean_maximum_load_factor, epoch)\n",
    "        #writer.add_scalar('optimality_gap/train_opt_gap', train_mean_maximum_load_factor/train_gt_load_factor - 1, epoch)\n",
    "        \n",
    "        if epoch % val_every == 0 or epoch == max_epochs-1:\n",
    "            # Validate\n",
    "            val_time, val_loss, val_err_edges, val_err_tour, val_err_tsp, val_pred_tour_len, val_gt_tour_len = test(net, config, epoch_bar, mode='val')\n",
    "            epoch_bar.write('val: ' + metrics_to_str(epoch, val_time, learning_rate, val_loss, val_err_edges, val_err_tour, val_err_tsp, val_pred_tour_len, val_gt_tour_len))\n",
    "            writer.add_scalar('loss/val_loss', val_loss, epoch)\n",
    "            writer.add_scalar('pred_tour_len/val_pred_tour_len', val_pred_tour_len, epoch)\n",
    "            writer.add_scalar('optimality_gap/val_opt_gap', val_pred_tour_len/val_gt_tour_len - 1, epoch)\n",
    "            \n",
    "            # Save checkpoint\n",
    "            if val_pred_tour_len < best_pred_tour_len:\n",
    "                best_pred_tour_len = val_pred_tour_len  # Update best prediction\n",
    "                torch.save({\n",
    "                    'epoch': epoch,\n",
    "                    'model_state_dict': net.state_dict(),\n",
    "                    'optimizer_state_dict': optimizer.state_dict(),\n",
    "                    'train_loss': train_loss,\n",
    "                    'val_loss': val_loss,\n",
    "                }, log_dir+\"best_val_checkpoint.tar\")\n",
    "            \n",
    "            # Update learning rate\n",
    "            if val_loss > 0.99 * val_loss_old:\n",
    "                learning_rate /= decay_rate\n",
    "                optimizer = update_learning_rate(optimizer, learning_rate)\n",
    "            \n",
    "            val_loss_old = val_loss  # Update old validation loss\n",
    "        \n",
    "        if epoch % test_every == 0 or epoch == max_epochs-1:\n",
    "\n",
    "            # Test\n",
    "            test_time, test_loss, test_err_edges, test_err_tour, test_err_tsp, test_pred_tour_len, test_gt_tour_len = test(net, config, epoch_bar, mode='test')\n",
    "            epoch_bar.write('test: ' + metrics_to_str(epoch, test_time, learning_rate, test_loss, test_err_edges, test_err_tour, test_err_tsp, test_pred_tour_len, test_gt_tour_len))\n",
    "            writer.add_scalar('loss/test_loss', test_loss, epoch)\n",
    "            writer.add_scalar('pred_tour_len/test_pred_tour_len', test_pred_tour_len, epoch)\n",
    "            writer.add_scalar('optimality_gap/test_opt_gap', test_pred_tour_len/test_gt_tour_len - 1, epoch)\n",
    "            \n",
    "        # Save training checkpoint at the end of epoch\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': net.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "            'val_loss': val_loss,\n",
    "        }, log_dir+\"last_train_checkpoint.tar\")\n",
    "        \n",
    "        # Save checkpoint after every 250 epochs\n",
    "        if epoch != 0 and (epoch % 250 == 0 or epoch == max_epochs-1):\n",
    "            torch.save({\n",
    "                'epoch': epoch,\n",
    "                'model_state_dict': net.state_dict(),\n",
    "                'optimizer_state_dict': optimizer.state_dict(),\n",
    "                'train_loss': train_loss,\n",
    "                'val_loss': val_loss,\n",
    "            }, log_dir+f\"checkpoint_epoch{epoch}.tar\")\n",
    "            \n",
    "    return net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[autoreload of utils.exact_solution failed: Traceback (most recent call last):\n",
      "  File \"/Users/osadashouta/miniconda3/envs/gcn-uelb-env/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 276, in check\n",
      "    superreload(m, reload, self.old_objects)\n",
      "  File \"/Users/osadashouta/miniconda3/envs/gcn-uelb-env/lib/python3.9/site-packages/IPython/extensions/autoreload.py\", line 475, in superreload\n",
      "    module = reload(module)\n",
      "  File \"/Users/osadashouta/miniconda3/envs/gcn-uelb-env/lib/python3.9/importlib/__init__.py\", line 168, in reload\n",
      "    raise ModuleNotFoundError(f\"spec not found for the module {name!r}\", name=name)\n",
      "ModuleNotFoundError: spec not found for the module 'utils.exact_solution'\n",
      "]\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "super(type, obj): obj must be an instance or subtype of type",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[97], line 3\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m viz_mode\u001b[38;5;241m==\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m:\n\u001b[1;32m      2\u001b[0m     \u001b[38;5;66;03m# del net\u001b[39;00m\n\u001b[0;32m----> 3\u001b[0m     net \u001b[38;5;241m=\u001b[39m \u001b[43mmain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[95], line 7\u001b[0m, in \u001b[0;36mmain\u001b[0;34m(config)\u001b[0m\n\u001b[1;32m      5\u001b[0m config_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mconfigs/default.json\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m      6\u001b[0m config \u001b[38;5;241m=\u001b[39m get_config(config_path)\n\u001b[0;32m----> 7\u001b[0m net \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mDataParallel(\u001b[43mResidualGatedGCNModel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypeFloat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtypeLong\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mis_available():\n\u001b[1;32m      9\u001b[0m     net\u001b[38;5;241m.\u001b[39mcuda()\n",
      "File \u001b[0;32m~/Desktop/Research/graph-convnet-uelb/models/gcn_model.py:17\u001b[0m, in \u001b[0;36mResidualGatedGCNModel.__init__\u001b[0;34m(self, config, dtypeFloat, dtypeLong)\u001b[0m\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__init__\u001b[39m(\u001b[38;5;28mself\u001b[39m, config, dtypeFloat, dtypeLong):\n\u001b[0;32m---> 17\u001b[0m     \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mResidualGatedGCNModel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39m\u001b[38;5;21m__init__\u001b[39m()\n\u001b[1;32m     18\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypeFloat \u001b[38;5;241m=\u001b[39m dtypeFloat\n\u001b[1;32m     19\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtypeLong \u001b[38;5;241m=\u001b[39m dtypeLong\n",
      "\u001b[0;31mTypeError\u001b[0m: super(type, obj): obj must be an instance or subtype of type"
     ]
    }
   ],
   "source": [
    "if viz_mode==False:\n",
    "    # del net\n",
    "    net = main(config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gcn-tsp-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.20"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
